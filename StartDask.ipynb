{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Dask on AzureML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.runconfig import RunConfiguration, MpiConfiguration\n",
    "from azureml.train.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='ncus-azureml', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copetersrg')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute(workspace=Workspace.create(name='ncus-azureml', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copetersrg'), name=dask-cluster, id=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourceGroups/copetersrg/providers/Microsoft.MachineLearningServices/workspaces/ncus-azureml/computes/dask-cluster, type=AmlCompute, provisioning_state=Succeeded, location=northcentralus, tags=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct = ws.compute_targets['dask-cluster']\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the Dask cluster using an Estimator with MpiConfiguration. Make sure the cluster is able to scale up to 10 nodes or change the `node_count` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = Estimator('dask', \n",
    "                compute_target=ct, \n",
    "                entry_script='startDask.py', \n",
    "                conda_dependencies_file='environment.yml', \n",
    "                script_params={'--datastore': ws.get_default_datastore()},\n",
    "                node_count=40,\n",
    "                distributed_training=MpiConfiguration())\n",
    "\n",
    "run = Experiment(ws, 'dask').submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe01d48cfb6427db589444ecc83ff03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'NOTSET',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/dask/runs/dask_1576287903_6e3e23ff?wsid=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourcegroups/copetersrg/workspaces/ncus-azureml\", \"run_id\": \"dask_1576287903_6e3e23ff\", \"run_properties\": {\"run_id\": \"dask_1576287903_6e3e23ff\", \"created_utc\": \"2019-12-14T01:45:05.100091Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"5690e524-e062-4175-8d1a-5aea87466f53\", \"azureml.git.repository_uri\": \"https://github.com/lostmygithubaccount/dask-examples.git\", \"mlflow.source.git.repoURL\": \"https://github.com/lostmygithubaccount/dask-examples.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"93bf733f269d5594284806162acf4e4c5d3e63f5\", \"mlflow.source.git.commit\": \"93bf733f269d5594284806162acf4e4c5d3e63f5\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_06c3a16e1965852b4bbdc5280ae06a32\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":38,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":40}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt?sv=2019-02-02&sr=b&sig=%2FWSkTB2YIIDm%2FtITmQPzIjlBe0taR5pcjTz2Pua%2B%2Fw8%3D&st=2019-12-14T02%3A11%3A01Z&se=2019-12-14T10%3A21%3A01Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt?sv=2019-02-02&sr=b&sig=0hu7uuJyg9UuEDE9aHADvy83dua7LxJQgnAo27jkOYs%3D&st=2019-12-14T02%3A11%3A01Z&se=2019-12-14T10%3A21%3A01Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt?sv=2019-02-02&sr=b&sig=K6ogTlxXYbsU2%2BDMYkOC9HkVMCBfjGeAW6q5ErzgIeA%3D&st=2019-12-14T02%3A11%3A01Z&se=2019-12-14T10%3A21%3A01Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt?sv=2019-02-02&sr=b&sig=OjH2Tm8FZ82j2ZxPczRyaO%2FhZgmNbjynsTPPgkekK%2BU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt?sv=2019-02-02&sr=b&sig=luX7od2IETlZQHCbxgN26D4dgiwPmZsPz4E%2BDQClotk%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt?sv=2019-02-02&sr=b&sig=rTSDqZKTIWzkRrb0xD%2BXMirlaodgDqHjvnel4XQpiZo%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt?sv=2019-02-02&sr=b&sig=P11ixr8qHDDFPWTQxwHe5pE4Exacm1xWT%2B3nKD655Lw%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt?sv=2019-02-02&sr=b&sig=JRb04%2B%2F%2BYfFGGWAEkkSV618RSeWDl373Cdj8YtUKgy4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt?sv=2019-02-02&sr=b&sig=Si1C1NjMJ69zfsolBYhQqyuh3we0Ph1%2FuojnR7Mn2l8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt?sv=2019-02-02&sr=b&sig=%2BlpFNqaF3Krr7xqRog5L3Bc6NA%2B7g83Q22hCh06rC1Q%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt?sv=2019-02-02&sr=b&sig=MOfetw2X1sdYqXmOLRp8F1MQhkNB4aYaqHYJn8Bdih0%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt?sv=2019-02-02&sr=b&sig=8La5dixtGZmG8%2F%2FIMrLhORDGPrGe%2BjQAdrZu1bdBCEc%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt?sv=2019-02-02&sr=b&sig=bRAHO%2BOxGrA5UxzDO6tvdK8zEroO%2FohBazUTx%2B8Z8Rs%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt?sv=2019-02-02&sr=b&sig=RaPiIPUIKqr8LRHHaKC2GNHqRA2gf2Ds32pEnmIoH%2FA%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt?sv=2019-02-02&sr=b&sig=7iPrIKDGyr2JFsi%2FQzPRSFoMSHR8s1zPlpVkCRL%2Bx0o%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt?sv=2019-02-02&sr=b&sig=EfhmuKAo%2BSUH859JNzqI7O3sUSLQrkocOZj%2BxnrznEU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt?sv=2019-02-02&sr=b&sig=u%2FrghBl08XZvRG5QAXR7aniPRZcSXT6bv0JWLoY9ZIU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt?sv=2019-02-02&sr=b&sig=86RX2VG1Q6XWNq6dZcZWLX%2FZvbfExlhAviOH1uu0Fek%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt?sv=2019-02-02&sr=b&sig=a6sK69T9NjRoAFdm%2Bn6oH6tHy91HiF%2BMkkbDnoDv3eI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt?sv=2019-02-02&sr=b&sig=DUf2DeIQP73R4dP%2BXbvc%2FcUTfM6KuDaPhW8xDk9LEkE%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt?sv=2019-02-02&sr=b&sig=F2Oq1fRubvS0Azab1eWlqXdnUSWzMtlzCcpokM3eAT8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt?sv=2019-02-02&sr=b&sig=IolvrV%2FH1eMX0TMICwT2JhQ33ptw7ttcd52Pqyo%2FZeg%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt?sv=2019-02-02&sr=b&sig=7Bh3qAswO85FKUAIJPpz5upGEQEJhvG9lbyaQptjUWo%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt?sv=2019-02-02&sr=b&sig=Li6eZ2xUpDA9ZTQ7O54LXa92%2FcTz22Mi3QXh2K%2BHRYc%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt?sv=2019-02-02&sr=b&sig=g%2FEbRNH8eZPOQg7ABiFAi2gqsbivSihAvnYKFKAEQLY%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt?sv=2019-02-02&sr=b&sig=UKn10wpyzyeVk1KbrxqnBpS5x4ZJRb6wz95fablvT7I%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt?sv=2019-02-02&sr=b&sig=Ue1Tvauny8DSy2RlBfyBDNYchyS4SgVRnKIM4CNmEAU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt?sv=2019-02-02&sr=b&sig=MMQaT7bMQAV3HYW4IFJXpEcCR2LGjsXTLXPK%2BCKfMJg%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt?sv=2019-02-02&sr=b&sig=xpxvzIyOCGk4Nmf4u7%2F6NzmHyqkOdMY9JRNPexXIpkU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt?sv=2019-02-02&sr=b&sig=A3%2FuWlZ4yL3yyBSrODFkRUUxA27CFBcu59B7qf3dB8U%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt?sv=2019-02-02&sr=b&sig=nJXBL8CW%2Fscu9Nsu3mGC4jiyS7pXLgguo9xuXxtF6uQ%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt?sv=2019-02-02&sr=b&sig=wu7XO80xlDofJV5zVvMzMpnPoZzfU3flY2Km5YFo27k%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt?sv=2019-02-02&sr=b&sig=xjpHkpf74xzP6trLyp%2FqToWZG%2Fs1DelEYD78PaRngqI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt?sv=2019-02-02&sr=b&sig=zPzxTMyiu%2FHg%2B9hMcnOQ%2BSesCNZISL9o5MM4QI2GMps%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt?sv=2019-02-02&sr=b&sig=oh4h%2F1hmKmTtO%2BOcwS%2FDpMiAMSBQyQFkTPFc%2FcSOpfU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt?sv=2019-02-02&sr=b&sig=b540Yi5N2BReTMktnH7Yl%2BksFIXb%2F9e5%2Bdr9VGPBAuA%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt?sv=2019-02-02&sr=b&sig=EXIBKQ%2BPQ17kIyGlIr069HoGzHCEfZsWIYRKW4F4HyQ%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt?sv=2019-02-02&sr=b&sig=uQ1aofBCBux0Zrtkh6icqqS0CEl%2FzXTAYMxGFOdf32M%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt?sv=2019-02-02&sr=b&sig=qutk9mOEE6CAOffaWUBY0YDzxSkJQr%2F5VIHjrYWqkPI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/55_azureml-execution-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt?sv=2019-02-02&sr=b&sig=U31ymq%2FQIx7ZksdMrWmne3QkhqVTyBLrRZShNNiG0jA%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt?sv=2019-02-02&sr=b&sig=b3dLeT1JSCjt8h6JSKQKCN2wabj73FQNsCx2gufYmmY%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt?sv=2019-02-02&sr=b&sig=mI791qNljS0NdTkuOsg4FyzQUOo9xIdvnTUD9AYf3nY%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt?sv=2019-02-02&sr=b&sig=OobxViJsbU6KQdoda5A5JMzUcmGtVTxATuMNhkBSmEU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt?sv=2019-02-02&sr=b&sig=E%2Bn38N8vurGP79yvFkhgUysUCAQlD5sKFyB3tDrRAwI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt?sv=2019-02-02&sr=b&sig=ryLuP7ycNPdGze4o%2F92DxzLIKt8x7LyUu66X%2FnUoNlU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt?sv=2019-02-02&sr=b&sig=%2B1iUza7F6ptX2zyHVE4P1wsT95Y3vAGhfd%2FLr6nn%2BR4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt?sv=2019-02-02&sr=b&sig=w0rOZGAM%2BYzqXUYX3KaGWKDYGpEGHnHtz9fTkn8SxO4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt?sv=2019-02-02&sr=b&sig=Dz1L9SufVac0qcuYSIhhlmFID%2BpRgpw1D36zSGY%2Fg4Y%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt?sv=2019-02-02&sr=b&sig=eFx%2B1QKAZV%2BKGZp8dyYYYy46CqBzrRV2UWbEGmee%2FE8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt?sv=2019-02-02&sr=b&sig=DDmKEAGUjgcg%2B%2BsFCAlY6uM%2BJ%2BoVkwA%2FgOhwrcQ4aRo%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt?sv=2019-02-02&sr=b&sig=a2zwgVe1ZRz16o8OnZBfULThbuaDtx4QSBCWPIo7Ius%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt?sv=2019-02-02&sr=b&sig=D%2BIdCWbJT7hJsPINFCh9ZQt7lKDNdDH2QR8lIrBjtwI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt?sv=2019-02-02&sr=b&sig=0mUB7%2B1GfjtfeUnmmS6U5FPdzCBgFpye%2BDYAIMrtfsk%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt?sv=2019-02-02&sr=b&sig=zoDvfcRZPc5aMnEiQIdyMXuiwx8plrFI2mIgJXa4ih4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt?sv=2019-02-02&sr=b&sig=PoK6OhrcecR9IkPEKm8cg2g7IC2RMrQPwebf8Ge7GBM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt?sv=2019-02-02&sr=b&sig=rQUNEcJHqJqvw1HYP00g6q7W9IBS%2BFPQc%2BcQW2MeVIA%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt?sv=2019-02-02&sr=b&sig=%2BJb%2BkS%2FOAOessTeYTKFMO0d5PnBs4wbRjTWgjMdDBdM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt?sv=2019-02-02&sr=b&sig=aXsRwN8%2F8cPvEMT%2BXvJ8FcrOXFZAJLnY%2BtgR6Je3GsY%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt?sv=2019-02-02&sr=b&sig=XcgS%2FEIiHyhb%2FeclSPU7WSE3WP0EjMG5AJDaRWJwEbM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt?sv=2019-02-02&sr=b&sig=X0NcwDzErSXaqcd39mQrSNxMARRjCQW8IOa4lapTvD4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt?sv=2019-02-02&sr=b&sig=25IS%2F6WzwWhMYfZGqjTUfM8JDPM5X%2Fv71wgp%2FcAUg6w%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt?sv=2019-02-02&sr=b&sig=%2FOgcbPJoCaZxBNIcEo9NLBTdPeQ9JLUXU%2BFzogCJVCI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt?sv=2019-02-02&sr=b&sig=9wNdWwxiK19%2BzNmzSQ47wZI3zN%2BPAzOPdhwM4pwoL9k%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt?sv=2019-02-02&sr=b&sig=J72yiHOYEpYwH7LNvGFJZR8UalgxIGwPo4u2ANWZtBU%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt?sv=2019-02-02&sr=b&sig=w8nfJRrltdZSLsZcBD3EDrEvh%2FsidOs6sA%2Ff6UnTEaE%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt?sv=2019-02-02&sr=b&sig=jSvZI7NZKWQsKhpnNMlW5ZHoskxJ5dX8Ca5ucnaAWtM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt?sv=2019-02-02&sr=b&sig=H5ZSVpfoxab4UEAwzj68i%2FlVYPxOza7e4yobVl2KSuA%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt?sv=2019-02-02&sr=b&sig=hkVVjyzvLrPLNY1PhXNmuJFwCiOIl%2B6pDKsSsmTDTE8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt?sv=2019-02-02&sr=b&sig=4H1S%2BDN0T4%2BP6kQVTRbmt9G8Un3NEMTZQzo5pOWwbjg%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt?sv=2019-02-02&sr=b&sig=dGDMobAADkPpodbS%2FXTxVo5fInPI7ErRHUFW37McTZM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt?sv=2019-02-02&sr=b&sig=iplC2VCkx7e2KKVoRoeM1CI5ixP%2FvceLCL7VqOc6fDs%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt?sv=2019-02-02&sr=b&sig=zdLlxWEXZKf7JhQ59Y31hafDwfYBc6EsSN1DYQ4ZHQg%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt?sv=2019-02-02&sr=b&sig=3XWhzzv3DND61ApCE3QRKZIloJeMsVGrY%2FqSI8oZ1cM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt?sv=2019-02-02&sr=b&sig=TdOTON9PgERVeTC%2Bn%2F18rXMMjMQ9s8OQ7Veo4wj4mPo%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt?sv=2019-02-02&sr=b&sig=IyQUH5THy5bM129Gsz3rLeKZ1sXFyuEPpQGrHPQ%2FSsI%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt?sv=2019-02-02&sr=b&sig=IB5%2BrF97yDKPR74JTQ0YOzf9zr5DWEUMrfcIBec8%2Bik%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt?sv=2019-02-02&sr=b&sig=KUvRrw4hgx07GNNPD5q23fxtI8uIzVX4LuecY4iBV0o%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt?sv=2019-02-02&sr=b&sig=b7idPo4pEC50yqeB7RuMhxysuMWZ4V%2BCnIW%2BWbe4Qd4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt?sv=2019-02-02&sr=b&sig=6U05Y6C%2B7UUqVbXvNf3xjMZBnqkckPcld7AorpOYwrQ%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/65_job_prep-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt?sv=2019-02-02&sr=b&sig=praE3XnQq1Si510COWbo82F78wqTeDXnskz5DCNl%2B6A%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_0.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_0.txt?sv=2019-02-02&sr=b&sig=eb0JymLzPB0u%2BlbRDtplCzXrn8wGpA5sY7Hredm2Mc4%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_1.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_1.txt?sv=2019-02-02&sr=b&sig=ssVKOR%2BkyvlOtRLWa%2FXawgt7JBbyBqWy%2FYISp46pBwQ%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_10.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_10.txt?sv=2019-02-02&sr=b&sig=WFTxhbYig9Jdg8RV1vJoz4aXElVbspTF5jF775vY8PQ%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_11.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_11.txt?sv=2019-02-02&sr=b&sig=LVLL8alzz28Lmr8%2FY7frh%2BYtMSTvUURnQKxN7daBs4Q%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_12.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_12.txt?sv=2019-02-02&sr=b&sig=7C6ss8k17BfZT%2B81lN5MSm9DbOmRdnhpDxpejkHisfs%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_13.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_13.txt?sv=2019-02-02&sr=b&sig=NVq82p%2BMlsTGJQWMoUTizsTUeROV%2Bpbk4XWyCsKlvU8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_14.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_14.txt?sv=2019-02-02&sr=b&sig=dZf2EzTQfUfk%2FeRCCYk89jX1oyEi6xWnAZ%2FJok5r49A%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_15.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_15.txt?sv=2019-02-02&sr=b&sig=oikux926hBKCVz7mp6%2FESZUft1Qz%2BJKQnZAsuLA4yyE%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_16.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_16.txt?sv=2019-02-02&sr=b&sig=duxhOZe2Uhb%2Fiur4fGbcaUfJHYm9w1mEanlEgtQo4jM%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_17.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_17.txt?sv=2019-02-02&sr=b&sig=MMmBD4erY8BCnawvvZ4n5mEyVtN8L2CAyvECVl94CVg%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_18.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_18.txt?sv=2019-02-02&sr=b&sig=L7naztWj5g9htPA%2FmaNgLBa2okADfnHYyPZHfaTDonc%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_19.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_19.txt?sv=2019-02-02&sr=b&sig=3N204wiBEMej6DrYcXzM1Z%2Fp3pKgVqCeUK%2B5qtHWvb0%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_2.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_2.txt?sv=2019-02-02&sr=b&sig=U83hVyhEPzL5DIWQzN%2FZqoAUbX%2FQMHzlRdV%2F1j4NPv8%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_20.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_20.txt?sv=2019-02-02&sr=b&sig=DLv3BnGlYcYAddBWoKlZELrcK4HUnstlTB1PmiW%2F1GE%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_21.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_21.txt?sv=2019-02-02&sr=b&sig=9ig7w%2BtnaHzXvCO0DvDvTwQg%2Fm4eMd47uhmsM95iRYc%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_22.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_22.txt?sv=2019-02-02&sr=b&sig=hu%2FhEZMvCQhBVzP%2BvNBPUaknzIiPVr2ahimzXquu4D0%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_23.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_23.txt?sv=2019-02-02&sr=b&sig=s8fi%2BV2lI70AWxINVLQ9c5iLAmZ8zMobQbpd%2Bemi8Qw%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_24.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_24.txt?sv=2019-02-02&sr=b&sig=umE5yzBwiFhqETBxUptbcRJjb7bHIMzRupqZIv795tk%3D&st=2019-12-14T02%3A11%3A02Z&se=2019-12-14T10%3A21%3A02Z&sp=r\", \"azureml-logs/70_driver_log_25.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_25.txt?sv=2019-02-02&sr=b&sig=FH8XoLOolgBBUU0T6qcn%2FsKWaeY%2BUddbXKD8s8QXx5Q%3D&st=2019-12-14T02%3A11%3A03Z&se=2019-12-14T10%3A21%3A03Z&sp=r\", \"azureml-logs/70_driver_log_26.txt\": \"https://ncusazureml3287372285.blob.core.windows.net/azureml/ExperimentRun/dcid.dask_1576287903_6e3e23ff/azureml-logs/70_driver_log_26.txt?sv=2019-02-02&sr=b&sig=BeZ645PPKrWWn3uYpyQZoWPdN2OlvorDpgTU1c1B8fk%3D&st=2019-12-14T02%3A11%3A03Z&se=2019-12-14T10%3A21%3A03Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/55_azureml-execution-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt\", \"azureml-logs/55_azureml-execution-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_00a4a408f6c33645ef497d013b5f06df7545ac264b57efd11b1e3a65fd84f26d_d.txt\", \"azureml-logs/65_job_prep-tvmps_1f2d23137990b9df3b8f23cc9ebea9b02273d80f397d4d4fe1541834ee2c4989_d.txt\", \"azureml-logs/65_job_prep-tvmps_2e3463b81f42a00055d59d872ce293f6309845ed92ed176a8123a3d0c3caa071_d.txt\", \"azureml-logs/65_job_prep-tvmps_3ef3d0f9a77e29b0fcdae438ebde782626b971ea05d29f22e1d5074e3f6cc9c3_d.txt\", \"azureml-logs/65_job_prep-tvmps_3fe5e22405f4ddc57506bd2504d3b4a26a2300c9ea3328957d826d6464cc7fd2_d.txt\", \"azureml-logs/65_job_prep-tvmps_faba3f0639385e2a43970b5176e0ffc1af98952779e768f8019568cd8d2ad13d_d.txt\", \"azureml-logs/65_job_prep-tvmps_f4a486096f4316028ed713890e57a07811f37c68f68688bd6d2c416e1b6f5198_d.txt\", \"azureml-logs/65_job_prep-tvmps_5c727b9a94724a6f2262e6cc49dfc76b84f279dbfbbd618d7f1ffaacbb54a521_d.txt\", \"azureml-logs/65_job_prep-tvmps_cb6b02d865eb30c399a012d1110176e7132a654707693d262ca1b91c1d861046_d.txt\", \"azureml-logs/65_job_prep-tvmps_c6aa6465ce752b0281d1ab4c9b095e18c9f7c08691806026d98860ca6c5b3220_d.txt\", \"azureml-logs/65_job_prep-tvmps_8df1c90da99c622992352a6e185aa7a918b751dcd5514d5ffe96efedb4989092_d.txt\", \"azureml-logs/65_job_prep-tvmps_8b2db129e7edcbabfc22773eaa19d4053480a04a5adf86aa1fb7964a3acb5574_d.txt\", \"azureml-logs/65_job_prep-tvmps_9bafab1a5bc0510bfce97ccbf83ad4d831eea49b184fc78e7a18f49d3fbbb2f2_d.txt\", \"azureml-logs/65_job_prep-tvmps_9a939652ab0db4635b8a1ce330227cb3be281d7a62af255046f931d1b49b2022_d.txt\", \"azureml-logs/65_job_prep-tvmps_10a08bc0332190363678f6e042b15c43c310c6161b275e285e47ec4825db3331_d.txt\", \"azureml-logs/65_job_prep-tvmps_016a3ea1ec984f802d7eabf72b44a46c6deb75ae8c3eccdc57c611370c594563_d.txt\", \"azureml-logs/65_job_prep-tvmps_de23d696b89609d12f2b284e017aec6b9fe0fd51abefaf9387498625390f6808_d.txt\", \"azureml-logs/65_job_prep-tvmps_ef28b6230a664e68b6177bed0589cb82aba840cc461d14f3db62f6bfd096a31c_d.txt\", \"azureml-logs/65_job_prep-tvmps_39cb2e21b9719e1c7aa5d34409cb83c9c37932ad9d6fc3d4db4ddc4c423c59e0_d.txt\", \"azureml-logs/65_job_prep-tvmps_dc42a50a125abc03b0f938ce171750a7efd7b1815e22b32b8f745c9d7f892ec7_d.txt\", \"azureml-logs/65_job_prep-tvmps_46afb9db00484e9b50cdc0641c4dbe381def287ec1c412781b7e968bfc286af2_d.txt\", \"azureml-logs/65_job_prep-tvmps_dc49e29cbf7dcd4565b641ddf81cc601a0544dd3d28552c207374e5a7ef3d145_d.txt\", \"azureml-logs/65_job_prep-tvmps_52ab66bca71ae15c4923028acb7c90ebc2199ed396961e9c2692377658b10e5e_d.txt\", \"azureml-logs/65_job_prep-tvmps_64cef227dcbf853f0f5ec5fc1b67aab8cd3b2f0f46f9e5f2699b397a0e9b45fb_d.txt\", \"azureml-logs/65_job_prep-tvmps_67d687590049be21729e3137f0f9fd1254d944bd8091d8be1c601e1f6cb16a9a_d.txt\", \"azureml-logs/65_job_prep-tvmps_73a92c24d00b71250ff9225add5576f86ecf910264fcda1ba5f0bf2891ae52b8_d.txt\", \"azureml-logs/65_job_prep-tvmps_fc76b89ef9db2b25acce7c1369ba52ddd75e12f1fe625b97c0e4ac04980d4202_d.txt\", \"azureml-logs/65_job_prep-tvmps_77b1ab26c5af79d9b2da562dcbc5df635a0f4e913ceaf36fbe693cacf7c706e5_d.txt\", \"azureml-logs/65_job_prep-tvmps_d78a3a3c31df92f97ff50218698b031674dc750ee00485360e19a6e129b83a6e_d.txt\", \"azureml-logs/65_job_prep-tvmps_ae82ca11f6f11174e3cb487acec75b0502f803f7234d69c6a61e3340359c538e_d.txt\", \"azureml-logs/65_job_prep-tvmps_b0115e9c7762dad749161c6e5c2230bd3cb4b6ff5d0af6ef4218222292a43d91_d.txt\", \"azureml-logs/65_job_prep-tvmps_451cdaa1e80d0144be021a0af6f649c3fd2d76f788b606e97b7fe8cf0b4bc335_d.txt\", \"azureml-logs/65_job_prep-tvmps_508ee11e73210d320620761bce6a0640295d2053ff639914e09f481a9d26ebb3_d.txt\", \"azureml-logs/65_job_prep-tvmps_668a8ca3dbeffe6b4233a068118386d03fa23e6664eb91aed5aa566b318d5969_d.txt\", \"azureml-logs/65_job_prep-tvmps_989f2cd90d2eace837dd4fb334dfc35291ff64f5f40e24fa569ec59dc4ac10cf_d.txt\", \"azureml-logs/65_job_prep-tvmps_997ea5d97d0f330021c18c52a597ed09338f51340b4e9bc2f597efc7bbeabaec_d.txt\", \"azureml-logs/65_job_prep-tvmps_a2382b2623a919497090e7780f549901603fb8b5d1eef62667abfae431388eda_d.txt\", \"azureml-logs/65_job_prep-tvmps_fca14767ecc2dec58865fa6d308f058320a4f5b1b44b453f0ce5d5912959b80a_d.txt\", \"azureml-logs/65_job_prep-tvmps_aa917422ebc96abca8535300228bc55b0f699687bf3d190a5db9a68455267dfa_d.txt\", \"azureml-logs/65_job_prep-tvmps_694079079bcec8f20ca0bbb71c9d35453e4259faa2e757456de4f08a6e08edcf_d.txt\"], [\"azureml-logs/70_driver_log_0.txt\", \"azureml-logs/70_driver_log_1.txt\", \"azureml-logs/70_driver_log_2.txt\", \"azureml-logs/70_driver_log_10.txt\", \"azureml-logs/70_driver_log_11.txt\", \"azureml-logs/70_driver_log_12.txt\", \"azureml-logs/70_driver_log_13.txt\", \"azureml-logs/70_driver_log_14.txt\", \"azureml-logs/70_driver_log_15.txt\", \"azureml-logs/70_driver_log_16.txt\", \"azureml-logs/70_driver_log_17.txt\", \"azureml-logs/70_driver_log_18.txt\", \"azureml-logs/70_driver_log_19.txt\", \"azureml-logs/70_driver_log_20.txt\", \"azureml-logs/70_driver_log_21.txt\", \"azureml-logs/70_driver_log_22.txt\", \"azureml-logs/70_driver_log_23.txt\", \"azureml-logs/70_driver_log_24.txt\", \"azureml-logs/70_driver_log_25.txt\", \"azureml-logs/70_driver_log_26.txt\"]], \"run_duration\": \"0:35:57\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [{\"name\": \"headnode\", \"run_id\": \"dask_1576287903_6e3e23ff\", \"categories\": [0], \"series\": [{\"data\": [\"10.1.0.5\"]}]}, {\"name\": \"cluster\", \"run_id\": \"dask_1576287903_6e3e23ff\", \"categories\": [0], \"series\": [{\"data\": [\"scheduler: 10.1.0.5:8786, dashboard: 10.1.0.5:8787\"]}]}, {\"name\": \"datastore\", \"run_id\": \"dask_1576287903_6e3e23ff\", \"categories\": [0], \"series\": [{\"data\": [\"/mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore\"]}]}], \"run_logs\": \"This is an MPI job. Rank:0\\nStarting the daemon thread to refresh tokens in background for process with pid = 215\\nEntering Run History Context Manager.\\n- my rank is  0\\n- my ip is  10.1.0.5\\n- scheduler is  10.1.0.5:8786\\n- dashboard is  10.1.0.5:8787\\nargs:  Namespace(datastore='/mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore', script=None)\\nunparsed:  []\\n- my rank is  0\\n- my ip is  10.1.0.5\\ndistributed.scheduler - INFO - -----------------------------------------------\\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.1.0.5:39007'\\ndistributed.scheduler - INFO - Local Directory:    /tmp/scheduler-4tl3lb0d\\ndistributed.scheduler - INFO - -----------------------------------------------\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO -   Scheduler at:       tcp://10.1.0.5:8786\\ndistributed.scheduler - INFO -   dashboard at:                     :8787\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:40907\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:40907\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:42725\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:42725\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.7:45891\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.7:45891\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.8:40939\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.8:40939\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:43415\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:43415\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:33151\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:33151\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.11:36241\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.11:36241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:36637\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:36637\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.27:46427\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.27:46427\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:33519\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:33519\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.42:37497\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.42:37497\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:34695\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:34695\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:40027\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:40027\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.41:45089\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.41:45089\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.31:42821\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.31:42821\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:45771\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:45771\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:35701\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:35701\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:36675\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:36675\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.43:38407\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.43:38407\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.6:40417\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.6:40417\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.23:41217\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.23:41217\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.15:46161\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.15:46161\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:38035\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:38035\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:40175\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:40175\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:33639\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:33639\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.44:44533\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.44:44533\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:32771\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:32771\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.40:38397\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.40:38397\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:44709\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:44709\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:44859\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:44859\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:36645\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:36645\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:46701\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:46701\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:43867\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:43867\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:40345\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:40345\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:33725\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:33725\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:34853\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:34853\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.39:44979\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.39:44979\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.19:44755\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.19:44755\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.1.0.5:39871\\ndistributed.worker - INFO -          Listening to:       tcp://10.1.0.5:39871\\ndistributed.worker - INFO -          dashboard at:             10.1.0.5:43669\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                          8\\ndistributed.worker - INFO -                Memory:                   59.08 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore/azureml/dask_1576287903_6e3e23ff/worker-kcat310g\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.5:39871\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.5:39871\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:43909\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:43909\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Receive client connection: Client-78fe7f90-1e14-11ea-8d92-000d3a616920\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.10:33639\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.10:33639\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.11:36241\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.11:36241\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.12:36675\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.12:36675\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:43415\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:43415\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:40027\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:40027\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.15:46161\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.15:46161\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.16:43909\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.16:43909\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.17:36645\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.17:36645\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:44859\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:44859\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.19:44755\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.19:44755\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.20:33725\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.20:33725\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.21:32771\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.21:32771\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:46701\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:46701\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.23:41217\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.23:41217\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.24:44709\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.24:44709\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.25:34853\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.25:34853\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:35701\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:35701\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.27:46427\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.27:46427\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.28:43867\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.28:43867\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:33151\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:33151\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:45771\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:45771\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.31:42821\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.31:42821\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.32:36637\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.32:36637\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.33:34695\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.33:34695\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:40175\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:40175\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.35:40907\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.35:40907\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.36:40345\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.36:40345\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.37:33519\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.37:33519\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.38:38035\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.38:38035\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.39:44979\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.39:44979\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.40:38397\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.40:38397\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.41:45089\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.41:45089\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.42:37497\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.42:37497\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.43:38407\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.43:38407\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.44:44533\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.44:44533\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.5:39871\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.5:39871\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.6:40417\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.6:40417\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.7:45891\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.7:45891\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.8:40939\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.8:40939\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.9:42725\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.9:42725\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:40907\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:40907\\ndistributed.worker - INFO - Stopping worker at tcp://10.1.0.5:39871\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.35:40907\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.35:40907\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.nanny - WARNING - Restarting worker\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:34485\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:34485\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.11:45361\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.11:45361\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:43755\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:43755\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:42289\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:42289\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.42:42633\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.42:42633\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.43:36585\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.43:36585\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:35267\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:35267\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:32991\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:32991\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:44501\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:44501\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:46435\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:46435\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:41525\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:41525\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.41:35827\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.41:35827\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:40797\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:40797\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.31:45951\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.31:45951\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:35331\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:35331\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.27:40783\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.27:40783\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.8:44043\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.8:44043\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:40097\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:40097\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.1.0.5:34897\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:35811\\ndistributed.worker - INFO -          Listening to:       tcp://10.1.0.5:34897\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:35811\\ndistributed.worker - INFO -          dashboard at:             10.1.0.5:39643\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                          8\\ndistributed.worker - INFO -                Memory:                   59.08 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore/azureml/dask_1576287903_6e3e23ff/worker-kuzzla9j\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.5:34897\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.5:34897\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:40623\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:40623\\ndistributed.core - INFO - Starting established connection\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:37979\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:37979\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.7:46605\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.7:46605\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.15:34609\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.15:34609\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:42203\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:42203\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:42395\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:42395\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:33853\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:33853\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:38893\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:38893\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:37183\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:37183\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.23:33347\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.23:33347\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:40241\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:40241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:34333\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:34333\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:43737\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:43737\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.44:33261\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.44:33261\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:43585\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:43585\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.6:34791\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.6:34791\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.40:39303\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.40:39303\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.39:36967\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.39:36967\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.19:36559\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.19:36559\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:37825\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:37825\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Receive client connection: Client-7ecb5a4a-1e14-11ea-8d92-000d3a616920\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.10:37183\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.10:37183\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.11:45361\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.11:45361\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.12:46435\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.12:46435\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:35267\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:35267\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:41525\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:41525\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.15:34609\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.15:34609\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.16:37825\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.16:37825\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.17:42395\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.17:42395\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:40097\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:40097\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.19:36559\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.19:36559\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.20:43585\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.20:43585\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.21:40623\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.21:40623\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:33853\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:33853\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.23:33347\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.23:33347\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.24:35811\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.24:35811\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.25:43737\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.25:43737\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:40241\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:40241\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.27:40783\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.27:40783\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.28:38893\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.28:38893\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:34485\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:34485\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:37979\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:37979\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.31:45951\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.31:45951\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.32:42289\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.32:42289\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.33:35331\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.33:35331\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:34333\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:34333\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.35:44501\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.35:44501\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.36:42203\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.36:42203\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.37:43755\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.37:43755\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.38:40797\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.38:40797\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.39:36967\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.39:36967\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.40:39303\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.40:39303\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.41:35827\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.41:35827\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.42:42633\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.42:42633\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.43:36585\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.43:36585\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.44:33261\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.44:33261\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.5:34897\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.5:34897\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.6:34791\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.6:34791\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.7:46605\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.7:46605\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.8:44043\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.8:44043\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.9:32991\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.9:32991\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:35267\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:35267\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:32991\\ndistributed.worker - INFO - Stopping worker at tcp://10.1.0.5:34897\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:32991\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:35267\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:35267\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.9:32991\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.9:32991\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:33775\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:33775\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:38639\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:38639\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:44223\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:44223\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.11:33427\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.11:33427\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:36919\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:36919\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:43901\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:43901\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:45015\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:45015\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.41:42781\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.41:42781\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:44637\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:44637\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:38411\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:38411\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.27:40019\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.27:40019\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:34431\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:34431\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.7:33843\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.7:33843\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:45317\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:45317\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:42947\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:42947\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.23:44743\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.23:44743\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.15:46131\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.15:46131\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:42575\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:42575\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.42:36327\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.42:36327\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:43257\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:43257\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:38207\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:38207\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:41805\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:41805\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:38009\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:38009\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.31:35379\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.31:35379\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.40:38927\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.40:38927\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:34215\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:34215\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:33373\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:33373\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.8:34489\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.8:34489\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.43:40893\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.43:40893\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.44:36535\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.44:36535\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:45895\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:45895\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:38771\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:38771\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.39:33813\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.39:33813\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:42019\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:42019\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.1.0.5:43921\\ndistributed.worker - INFO -          Listening to:       tcp://10.1.0.5:43921\\ndistributed.worker - INFO -          dashboard at:             10.1.0.5:44745\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                          8\\ndistributed.worker - INFO -                Memory:                   59.08 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore/azureml/dask_1576287903_6e3e23ff/worker-5i8id6zh\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.5:43921\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.5:43921\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:39427\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:39427\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.6:34903\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.6:34903\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:38221\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:38221\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.19:45013\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.19:45013\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:46737\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:46737\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.utils - ERROR - 'start'\\nTraceback (most recent call last):\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/distributed/utils.py\\\", line 663, in log_errors\\n    yield\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/distributed/dashboard/components/shared.py\\\", line 312, in update\\n    ts = metadata[\\\"keys\\\"][self.key]\\nKeyError: 'start'\\ntornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f25cf46f940>>, <Future finished exception=KeyError('start',)>)\\nTraceback (most recent call last):\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/tornado/ioloop.py\\\", line 743, in _run_callback\\n    ret = callback()\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/tornado/ioloop.py\\\", line 767, in _discard_future_result\\n    future.result()\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/tornado/gen.py\\\", line 748, in run\\n    yielded = self.gen.send(value)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/server/session.py\\\", line 70, in _needs_document_lock_wrapper\\n    result = yield yield_for_all_futures(func(self, *args, **kwargs))\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/server/session.py\\\", line 191, in with_document_locked\\n    return func(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/document/document.py\\\", line 1127, in wrapper\\n    return doc._with_self_as_curdoc(invoke)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/document/document.py\\\", line 1113, in _with_self_as_curdoc\\n    return f()\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/document/document.py\\\", line 1126, in invoke\\n    return f(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/document/document.py\\\", line 916, in remove_then_invoke\\n    return callback(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/distributed/dashboard/components/shared.py\\\", line 333, in <lambda>\\n    self.doc().add_next_tick_callback(lambda: self.update(prof, metadata))\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/bokeh/core/property/validation.py\\\", line 97, in func\\n    return input_function(*args, **kwargs)\\n  File \\\"/azureml-envs/azureml_52825fcb9dfe0fb325b0c7d8d9df71c6/lib/python3.6/site-packages/distributed/dashboard/components/shared.py\\\", line 312, in update\\n    ts = metadata[\\\"keys\\\"][self.key]\\nKeyError: 'start'\\ndistributed.scheduler - INFO - Receive client connection: Client-1a289830-1e17-11ea-8d92-000d3a616920\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.10:38207\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.10:38207\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.11:33427\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.11:33427\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.12:45317\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.12:45317\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:38639\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:38639\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:42575\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:42575\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.15:46131\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.15:46131\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.16:46737\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.16:46737\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.17:43257\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.17:43257\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:34215\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:34215\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.19:45013\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.19:45013\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.20:39427\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.20:39427\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.21:42947\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.21:42947\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:33373\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:33373\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.23:44743\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.23:44743\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.24:38771\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.24:38771\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.25:34431\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.25:34431\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:38009\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:38009\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.27:40019\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.27:40019\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.28:41805\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.28:41805\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:33775\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:33775\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:44637\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:44637\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.31:35379\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.31:35379\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.32:38411\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.32:38411\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.33:36919\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.33:36919\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:45895\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:45895\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.35:44223\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.35:44223\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.36:42019\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.36:42019\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.37:43901\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.37:43901\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.38:38221\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.38:38221\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.39:33813\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.39:33813\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.40:38927\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.40:38927\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.41:42781\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.41:42781\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.42:36327\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.42:36327\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.43:40893\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.43:40893\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.44:36535\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.44:36535\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.5:43921\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.5:43921\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.6:34903\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.6:34903\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.7:33843\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.7:33843\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.8:34489\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.8:34489\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.9:45015\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.9:45015\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:33775\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:33775\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:38639\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:38639\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO - Stopping worker at tcp://10.1.0.5:43921\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:38639\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:38639\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:33775\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:33775\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:35731\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:35731\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:35595\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:35595\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:33963\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:33963\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:36847\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:36847\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.11:43725\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.11:43725\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.27:39849\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.27:39849\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:35451\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:35451\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:34961\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:34961\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.41:34585\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.41:34585\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:36565\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:36565\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:46847\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:46847\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:39637\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:39637\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.42:35981\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.42:35981\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.31:34899\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.31:34899\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.8:42911\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.8:42911\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:35463\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:35463\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:36165\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:36165\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.43:39967\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.43:39967\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.15:33807\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.15:33807\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:38451\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:38451\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.7:44057\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.7:44057\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.6:39359\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.6:39359\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.23:39557\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.23:39557\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:33187\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:33187\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:46205\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:46205\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.40:43917\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.40:43917\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.39:33309\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.39:33309\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:36205\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:36205\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:37241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:42501\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:42501\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:40975\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:40975\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:39465\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:39465\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:44689\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:44689\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:44355\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:44355\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.1.0.5:44869\\ndistributed.worker - INFO -          Listening to:       tcp://10.1.0.5:44869\\ndistributed.worker - INFO -          dashboard at:             10.1.0.5:37107\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                          8\\ndistributed.worker - INFO -                Memory:                   59.08 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore/azureml/dask_1576287903_6e3e23ff/worker-r6eoe26y\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.44:33281\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.44:33281\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.19:35085\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.19:35085\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.5:44869\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.5:44869\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:40519\\ndistributed.worker - INFO -         Registered to:        tcp://10.1.0.5:8786\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:40519\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:37577\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:37577\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:39047\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:39047\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.10:44355\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.10:44355\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:44355\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:44355\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.12:36565\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.12:36565\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:36565\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:36565\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:46205\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:46205\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:46205\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:46205\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:37241\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:37241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:37241\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:37241\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.16:39047\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.16:39047\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:39047\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:39047\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.17:38451\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.17:38451\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:38451\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:38451\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:39465\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:39465\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:39465\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:39465\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Receive client connection: Client-78036f7a-1e17-11ea-8d92-000d3a616920\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Send lost future signal to clients\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.10:44355\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.10:44355\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.11:43725\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.11:43725\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.12:36565\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.12:36565\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.13:46205\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.13:46205\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.14:37241\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.14:37241\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.15:33807\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.15:33807\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.16:39047\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.16:39047\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.17:38451\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.17:38451\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:39465\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:39465\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.19:35085\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.19:35085\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.20:40519\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.20:40519\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.21:35463\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.21:35463\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:36165\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:36165\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.23:39557\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.23:39557\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.24:33187\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.24:33187\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.25:37577\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.25:37577\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:39637\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:39637\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.27:39849\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.27:39849\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.28:44689\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.28:44689\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:35731\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:35731\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:33963\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:33963\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.31:34899\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.31:34899\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.32:35451\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.32:35451\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.33:36847\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.33:36847\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:36205\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:36205\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.35:34961\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.35:34961\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.36:40975\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.36:40975\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.37:46847\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.37:46847\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.38:42501\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.38:42501\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.39:33309\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.39:33309\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.40:43917\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.40:43917\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.41:34585\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.41:34585\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.42:35981\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.42:35981\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.43:39967\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.43:39967\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.44:33281\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.44:33281\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.5:44869\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.5:44869\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.6:39359\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.6:39359\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.7:44057\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.7:44057\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.8:42911\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.8:42911\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.9:35595\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.9:35595\\ndistributed.scheduler - INFO - Lost all workers\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.worker - INFO - Stopping worker at tcp://10.1.0.5:44869\\ndistributed.nanny - WARNING - Restarting worker\\ndistributed.scheduler - INFO - Register tcp://10.1.0.27:34289\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.27:34289\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.13:41897\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.13:41897\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.35:34169\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.35:34169\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:38369\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:38369\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:46185\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:46185\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:42663\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:42663\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.9:46857\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.9:46857\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.11:35447\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.11:35447\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.41:37635\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.41:37635\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:39891\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:39891\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:37189\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:37189\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:46447\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:46447\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.8:40171\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.8:40171\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:41295\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:41295\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.10:41201\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.10:41201\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.42:41373\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.42:41373\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.15:33857\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.15:33857\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.23:40145\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.23:40145\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:42685\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:42685\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.12:41957\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.12:41957\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:34907\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:34907\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:36905\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:36905\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.6:36493\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.6:36493\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:33937\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:33937\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.43:38637\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.43:38637\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.44:44027\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.44:44027\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.40:36365\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.40:36365\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.31:37025\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.31:37025\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.39:39577\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.39:39577\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.17:39289\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.17:39289\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:39031\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:39031\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:33429\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:33429\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.14:33973\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.14:33973\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:34997\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:34997\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:46861\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:46861\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.7:33969\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.7:33969\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.16:45183\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.16:45183\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.19:39155\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.19:39155\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:37345\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:37345\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -       Start worker at:       tcp://10.1.0.5:33641\\ndistributed.worker - INFO -          Listening to:       tcp://10.1.0.5:33641\\ndistributed.worker - INFO -          dashboard at:             10.1.0.5:45083\\ndistributed.worker - INFO - Waiting to connect to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.worker - INFO -               Threads:                          8\\ndistributed.worker - INFO -                Memory:                   59.08 GB\\ndistributed.worker - INFO -       Local Directory: /mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore/azureml/dask_1576287903_6e3e23ff/worker-zdb5wnk7\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.scheduler - INFO - Register tcp://10.1.0.5:33641\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.5:33641\\ndistributed.core - INFO - Starting established connection\\ndistributed.worker - INFO -         Registered to:        tcp://10.1.0.5:8786\\ndistributed.worker - INFO - -------------------------------------------------\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Clear task state\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.18:34907\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.18:34907\\ndistributed.scheduler - INFO - Register tcp://10.1.0.18:34907\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.18:34907\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.20:33937\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.20:33937\\ndistributed.scheduler - INFO - Register tcp://10.1.0.20:33937\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.20:33937\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.21:42685\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.21:42685\\ndistributed.scheduler - INFO - Register tcp://10.1.0.21:42685\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.21:42685\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:41295\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:41295\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:41295\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:41295\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.22:41295\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.22:41295\\ndistributed.scheduler - INFO - Register tcp://10.1.0.22:41295\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.22:41295\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.24:39031\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.24:39031\\ndistributed.scheduler - INFO - Register tcp://10.1.0.24:39031\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.24:39031\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.25:36905\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.25:36905\\ndistributed.scheduler - INFO - Register tcp://10.1.0.25:36905\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.25:36905\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:46447\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:46447\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:46447\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:46447\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.26:46447\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.26:46447\\ndistributed.scheduler - INFO - Register tcp://10.1.0.26:46447\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.26:46447\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.28:34997\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.28:34997\\ndistributed.scheduler - INFO - Register tcp://10.1.0.28:34997\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.28:34997\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.29:38369\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.29:38369\\ndistributed.scheduler - INFO - Register tcp://10.1.0.29:38369\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.29:38369\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:46185\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:46185\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:46185\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:46185\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.30:46185\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.30:46185\\ndistributed.scheduler - INFO - Register tcp://10.1.0.30:46185\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.30:46185\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.32:37189\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.32:37189\\ndistributed.scheduler - INFO - Register tcp://10.1.0.32:37189\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.32:37189\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.33:39891\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.33:39891\\ndistributed.scheduler - INFO - Register tcp://10.1.0.33:39891\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.33:39891\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:33429\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:33429\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:33429\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:33429\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.34:33429\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.34:33429\\ndistributed.scheduler - INFO - Register tcp://10.1.0.34:33429\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.34:33429\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.36:37345\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.36:37345\\ndistributed.scheduler - INFO - Register tcp://10.1.0.36:37345\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.36:37345\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.37:42663\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.37:42663\\ndistributed.scheduler - INFO - Register tcp://10.1.0.37:42663\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.37:42663\\ndistributed.core - INFO - Starting established connection\\ndistributed.scheduler - INFO - Remove worker tcp://10.1.0.38:46861\\ndistributed.core - INFO - Removing comms to tcp://10.1.0.38:46861\\ndistributed.scheduler - INFO - Register tcp://10.1.0.38:46861\\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.1.0.38:46861\\ndistributed.core - INFO - Starting established connection\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"NOTSET\", \"sdk_version\": \"1.0.76\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'headnode': '10.1.0.5', 'cluster': 'scheduler: 10.1.0.5:8786, dashboard: 10.1.0.5:8787', 'datastore': '/mnt/batch/tasks/shared/LS_root/jobs/ncus-azureml/azureml/dask_1576287903_6e3e23ff/mounts/workspaceblobstore'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while run.get_status() != 'Canceled' and 'headnode' not in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "if run.get_status() == 'Canceled':\n",
    "    print('Run was canceled')\n",
    "else:\n",
    "    headnode = run.get_metrics()['headnode']\n",
    "    print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.0.5:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.0.5:8787/status' target='_blank'>http://10.1.0.5:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>40</li>\n",
       "  <li><b>Cores: </b>320</li>\n",
       "  <li><b>Memory: </b>2.36 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.0.5:8786' processes=34 threads=272, memory=2.01 TB>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "c = Client(f'tcp://{headnode}:8786')\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the port-forwarding from Compute Instance to Dask Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh daskuser@10.1.0.5 -L 8788:10.1.0.5:8787\n"
     ]
    }
   ],
   "source": [
    "print(f'ssh daskuser@{headnode} -L 8788:{headnode}:8787')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to leave the terminal tab open to keep the port-forward running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some jobs on the cluster\n",
    "If you are able to see the Bokeh app, it is time to use the cluster. Thanks to the port forward, the scheduler appears to the notebook VM at `tcp://localhost:8786`. You should see 10 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from azureml.opendatasets import NoaaIsdWeather\n",
    "\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2010, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [dask.delayed(NoaaIsdWeather(start+timedelta(days=i), start+timedelta(days=i+1)).to_pandas_dataframe) for i in range(365*10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target paths: ['/year=2010/month=1/']\n",
      "Looking for parquet files...\n",
      "Reading them into Pandas dataframe...\n",
      "Reading ISDWeather/year=2010/month=1/part-00009-tid-2760641154746282667-d6a50598-0478-455f-9217-ce4afb96a9bb-36.c000.snappy.parquet under container isdweatherdatacontainer\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usaf</th>\n",
       "      <th>wban</th>\n",
       "      <th>datetime</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>windAngle</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>temperature</th>\n",
       "      <th>seaLvlPressure</th>\n",
       "      <th>...</th>\n",
       "      <th>pastWeatherIndicator</th>\n",
       "      <th>precipTime</th>\n",
       "      <th>precipDepth</th>\n",
       "      <th>snowDepth</th>\n",
       "      <th>stationName</th>\n",
       "      <th>countryOrRegion</th>\n",
       "      <th>p_k</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999999</td>\n",
       "      <td>63858</td>\n",
       "      <td>2010-01-01 00:00:00</td>\n",
       "      <td>32.457</td>\n",
       "      <td>-87.242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SELMA 13 WNW</td>\n",
       "      <td>US</td>\n",
       "      <td>999999-63858</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999999</td>\n",
       "      <td>63858</td>\n",
       "      <td>2010-01-01 00:05:00</td>\n",
       "      <td>32.457</td>\n",
       "      <td>-87.242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SELMA 13 WNW</td>\n",
       "      <td>US</td>\n",
       "      <td>999999-63858</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999999</td>\n",
       "      <td>63858</td>\n",
       "      <td>2010-01-01 00:10:00</td>\n",
       "      <td>32.457</td>\n",
       "      <td>-87.242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SELMA 13 WNW</td>\n",
       "      <td>US</td>\n",
       "      <td>999999-63858</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>999999</td>\n",
       "      <td>63858</td>\n",
       "      <td>2010-01-01 00:15:00</td>\n",
       "      <td>32.457</td>\n",
       "      <td>-87.242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SELMA 13 WNW</td>\n",
       "      <td>US</td>\n",
       "      <td>999999-63858</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>999999</td>\n",
       "      <td>63858</td>\n",
       "      <td>2010-01-01 00:20:00</td>\n",
       "      <td>32.457</td>\n",
       "      <td>-87.242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SELMA 13 WNW</td>\n",
       "      <td>US</td>\n",
       "      <td>999999-63858</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     usaf   wban            datetime  latitude  longitude  elevation  \\\n",
       "0  999999  63858 2010-01-01 00:00:00    32.457    -87.242       59.0   \n",
       "1  999999  63858 2010-01-01 00:05:00    32.457    -87.242       59.0   \n",
       "2  999999  63858 2010-01-01 00:10:00    32.457    -87.242       59.0   \n",
       "3  999999  63858 2010-01-01 00:15:00    32.457    -87.242       59.0   \n",
       "4  999999  63858 2010-01-01 00:20:00    32.457    -87.242       59.0   \n",
       "\n",
       "   windAngle  windSpeed  temperature  seaLvlPressure   ...     \\\n",
       "0        0.0        0.0          6.2             NaN   ...      \n",
       "1        NaN        NaN          6.3             NaN   ...      \n",
       "2        NaN        NaN          6.3             NaN   ...      \n",
       "3        NaN        NaN          6.4             NaN   ...      \n",
       "4        NaN        NaN          5.6             NaN   ...      \n",
       "\n",
       "  pastWeatherIndicator  precipTime  precipDepth  snowDepth   stationName  \\\n",
       "0                  NaN         1.0          0.0        NaN  SELMA 13 WNW   \n",
       "1                  NaN         NaN          NaN        NaN  SELMA 13 WNW   \n",
       "2                  NaN         NaN          NaN        NaN  SELMA 13 WNW   \n",
       "3                  NaN         NaN          NaN        NaN  SELMA 13 WNW   \n",
       "4                  NaN         NaN          NaN        NaN  SELMA 13 WNW   \n",
       "\n",
       "   countryOrRegion           p_k  year day  version  \n",
       "0               US  999999-63858  2010   1      1.0  \n",
       "1               US  999999-63858  2010   1      1.0  \n",
       "2               US  999999-63858  2010   1      1.0  \n",
       "3               US  999999-63858  2010   1      1.0  \n",
       "4               US  999999-63858  2010   1      1.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = NoaaIsdWeather(start, start+timedelta(days=1)).to_pandas_dataframe()\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KilledWorker",
     "evalue": "('to_pandas_dataframe-f8ac14f6-5cac-4fe6-9745-16e7676c469c', <Worker 'tcp://10.1.0.38:46861', memory: 0, processing: 1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-98220c5c153f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m             )\n\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             return sync(\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             )\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: ('to_pandas_dataframe-f8ac14f6-5cac-4fe6-9745-16e7676c469c', <Worker 'tcp://10.1.0.38:46861', memory: 0, processing: 1>)"
     ]
    }
   ],
   "source": [
    "fns[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KilledWorker",
     "evalue": "('to_pandas_dataframe-f8ac14f6-5cac-4fe6-9745-16e7676c469c', <Worker 'tcp://10.1.0.34:33429', memory: 0, processing: 1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKilledWorker\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-4c8c9e694ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_delayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/dask/dataframe/io/io.py\u001b[0m in \u001b[0;36mfrom_delayed\u001b[0;34m(dfs, meta, divisions, prefix, verify_meta)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \"\"\"\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2537\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m             )\n\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             return sync(\n\u001b[0;32m--> 756\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m             )\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1693\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKilledWorker\u001b[0m: ('to_pandas_dataframe-f8ac14f6-5cac-4fe6-9745-16e7676c469c', <Worker 'tcp://10.1.0.34:33429', memory: 0, processing: 1>)"
     ]
    }
   ],
   "source": [
    "df = dd.from_delayed(fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.0.5:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.0.5:8787/status' target='_blank'>http://10.1.0.5:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>40</li>\n",
       "  <li><b>Cores: </b>320</li>\n",
       "  <li><b>Memory: </b>2.36 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.0.5:8786' processes=40 threads=320, memory=2.36 TB>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dataflow in module azureml.dataprep.api.dataflow object:\n",
      "\n",
      "class Dataflow(builtins.object)\n",
      " |  A Dataflow represents a series of lazily-evaluated, immutable operations on data.\n",
      " |      It is only an execution plan. No data is loaded from the source until you get data from the Dataflow using one of `head`, `to_pandas_dataframe`, `get_profile` or the write methods.\n",
      " |  \n",
      " |  .. remarks::\n",
      " |  \n",
      " |      Dataflows are usually created by supplying a data source. Once the data source has been provided, operations\n",
      " |          can be added by invoking the different transformation methods available on this class. The result of adding\n",
      " |          an operation to a Dataflow is always a new Dataflow.\n",
      " |  \n",
      " |      The actual loading of the data and execution of the transformations is delayed as much as possible and will not\n",
      " |          occur until a 'pull' takes place. A pull is the action of reading data from a Dataflow, whether by asking to\n",
      " |          look at the first N records in it or by transferring the data in the Dataflow to another storage mechanism\n",
      " |          (a Pandas Dataframe, a CSV file, or a Spark Dataframe).\n",
      " |  \n",
      " |      The operations available on the Dataflow are runtime-agnostic. This allows the transformation pipelines\n",
      " |          contained in them to be portable between a regular Python environment and Spark.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      # Will fold the right Dataflow into the left by appending the rights steps to the lefts.\n",
      " |  \n",
      " |  __deepcopy__(self, memodict=None)\n",
      " |      # Steps are immutable so we don't need to create a full deepcopy of them when cloning Dataflows.\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, engine_api:azureml.dataprep.api.engineapi.api.EngineAPI, steps:List[azureml.dataprep.api.step.Step]=None, meta:Dict[str, str]=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, newstate)\n",
      " |  \n",
      " |  add_column(self, expression:azureml.dataprep.api.expressions.Expression, new_column_name:str, prior_column:str) -> 'Dataflow'\n",
      " |      Adds a new column to the dataset. The values in the new column will be the result of invoking the specified\n",
      " |      expression.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          Expressions are built using the expression builders in the expressions module and the functions in\n",
      " |          the functions module. The resulting expression will be lazily evaluated for each record when a data pull\n",
      " |          occurs and not where it is defined.\n",
      " |      \n",
      " |      :param expression: The expression to evaluate to generate the values in the column.\n",
      " |      :param new_column_name: The name of the new column.\n",
      " |      :param prior_column: The name of the column after which the new column should be added. The default is to add\n",
      " |          the new column as the last column.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  add_step(self, step_type:str, arguments:Dict[str, Any], local_data:Dict[str, Any]=None) -> 'Dataflow'\n",
      " |  \n",
      " |  append_columns(self, dataflows:List[_ForwardRef('DataflowReference')], parallelize:bool=True) -> 'Dataflow'\n",
      " |      Appends the columns from the referenced dataflows to the current one. Duplicate columns will result in failure.\n",
      " |      \n",
      " |      :param dataflows: The dataflows to append.\n",
      " |      :param parallelize: Whether to parallelize the operation. If true, the data for all inputs will be loaded into memory.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  append_rows(self, dataflows:List[_ForwardRef('DataflowReference')]) -> 'Dataflow'\n",
      " |      Appends the records in the specified dataflows to the current one. If the schemas of the dataflows are distinct, this will result in records\n",
      " |          with different schemas.\n",
      " |      \n",
      " |      :param dataflows: The dataflows to append.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  assert_value(self, columns:~MultiColumnSelection, expression:azureml.dataprep.api.expressions.Expression, policy:azureml.dataprep.api.engineapi.typedefinitions.AssertPolicy=<AssertPolicy.ERRORVALUE: 1>, error_code:str='AssertionFailed') -> 'Dataflow'\n",
      " |      Ensures that values in the specified columns satisfy the provided expression. This is useful to identify anomalies in the dataset\n",
      " |          and avoid broken pipelines by handling assertion errors in a clean way.\n",
      " |      \n",
      " |      :param columns: Columns to apply evaluation to.\n",
      " |      :param expression: Expression that has to be evaluated to be True for the value to be kept.\n",
      " |      :param policy: Action to take when expression is evaluated to False. Options are `FAILEXECUTION` and `ERRORVALUE`.\n",
      " |          `FAILEXECUTION` ensures that any data that violates the assertion expression during execution will immediately fail the job. This is useful to save computing resources and time.\n",
      " |          `ERRORVALUE` captures any data that violates the assertion expression by replacing it with error_code. This allows you to handle these error values by either filtering or replacing them.\n",
      " |      :param error_code: Error message to use to replace values failing the assertion or failing an execution.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  cache(self, directory_path:str) -> 'Dataflow'\n",
      " |      Pulls all the records from this Dataflow and cache the result to disk.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          This is very useful when data is accessed repeatedly, as future executions will reuse\n",
      " |          the cached result without pulling the same Dataflow again.\n",
      " |      \n",
      " |      :param directory_path: The directory to save cache files.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  clip(self, columns:~MultiColumnSelection, lower:Union[float, NoneType]=None, upper:Union[float, NoneType]=None, use_values:bool=True) -> 'Dataflow'\n",
      " |      Clips values so that all values are between the lower and upper boundaries.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param lower: All values lower than this value will be set to this value.\n",
      " |      :param upper: All values higher than this value will be set to this value.\n",
      " |      :param use_values: If true, values outside boundaries will be set to the boundary values. If false, those values will be set to null.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  convert_unix_timestamp_to_datetime(self, columns:~MultiColumnSelection, use_seconds:bool=False) -> 'Dataflow'\n",
      " |      Converts the specified column to DateTime values by treating the existing value as a Unix timestamp.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param use_seconds: Whether to use seconds as the resolution. Milliseconds are used if false.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  derive_column_by_example(self, source_columns:~SourceColumns, new_column_name:str, example_data:~ExampleData) -> 'Dataflow'\n",
      " |      Inserts a column by learning a program based on a set of source columns and provided examples.\n",
      " |          Dataprep will attempt to achieve the intended derivation inferred from provided examples.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          If you need more control of examples and generated program, create DeriveColumnByExampleBuilder instead.\n",
      " |      \n",
      " |      :param source_columns: Names of the columns from which the new column will be derived.\n",
      " |      :param new_column_name: Name of the new column to add.\n",
      " |      :param example_data: Examples to use as input for program generation.\n",
      " |          In case there is only one column to be used as source, examples could be Tuples of source value and intended\n",
      " |          target value. For example, you can have \"example_data=[(\"2013-08-22\", \"Thursday\"), (\"2013-11-03\", \"Sunday\")]\".\n",
      " |          When multiple columns should be considered as source, each example should be a Tuple of dict-like sources\n",
      " |          and intended target value, where sources have column names as keys and column values as values.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  distinct(self, columns:~MultiColumnSelection) -> 'Dataflow'\n",
      " |      Filters out records that contain duplicate values in the specified columns, leaving only a single instance.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  distinct_rows(self) -> 'Dataflow'\n",
      " |      Filters out records that contain duplicate values in all columns, leaving only a single instance.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  drop_columns(self, columns:~MultiColumnSelection) -> 'Dataflow'\n",
      " |      Drops the specified columns.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  drop_errors(self, columns:~MultiColumnSelection, column_relationship:azureml.dataprep.api.engineapi.typedefinitions.ColumnRelationship=<ColumnRelationship.ALL: 0>) -> 'Dataflow'\n",
      " |      Drops rows where all or any of the selected columns are an Error.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param column_relationship: Whether all or any of the selected columns must be an Error.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  drop_nulls(self, columns:~MultiColumnSelection, column_relationship:azureml.dataprep.api.engineapi.typedefinitions.ColumnRelationship=<ColumnRelationship.ALL: 0>) -> 'Dataflow'\n",
      " |      Drops rows where all or any of the selected columns are null.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param column_relationship: Whether all or any of the selected columns must be null.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  duplicate_column(self, column_pairs:Dict[str, str]) -> 'Dataflow'\n",
      " |      Creates new columns that are duplicates of the specified source columns.\n",
      " |      \n",
      " |      :param column_pairs: Mapping of the columns to duplicate to their new names.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  error(self, columns:~MultiColumnSelection, find:Any, error_code:str) -> 'Dataflow'\n",
      " |      Creates errors in a column for values that match the specified search value.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The following types are supported for the find argument: str, int, float,\n",
      " |              datetime.datetime, and bool.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param find: The value to find, or None.\n",
      " |      :param error_code: The error code to use in new errors, or None.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  execute_inspector(self, inspector:azureml.dataprep.api.inspector.BaseInspector) -> azureml.dataprep.api.engineapi.typedefinitions.ExecuteInspectorCommonResponse\n",
      " |  \n",
      " |  execute_inspectors(self, inspectors:List[azureml.dataprep.api.inspector.BaseInspector]) -> Dict[azureml.dataprep.api.engineapi.typedefinitions.InspectorArguments, azureml.dataprep.api.engineapi.typedefinitions.ExecuteInspectorCommonResponse]\n",
      " |  \n",
      " |  extract_error_details(self, column:str, error_value_column:str, extract_error_code:bool=False, error_code_column:Union[str, NoneType]=None) -> 'Dataflow'\n",
      " |      Extracts the error details from error values into a new column.\n",
      " |      \n",
      " |      :param column: The source column.\n",
      " |      :param error_value_column: Name of a column to hold the original value of the error.\n",
      " |      :param extract_error_code: Whether to also extract the error code.\n",
      " |      :param error_code_column: Name of a column to hold the error code.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  fill_errors(self, columns:~MultiColumnSelection, fill_with:Any) -> 'Dataflow'\n",
      " |      Fills all errors in a column with the specified value.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The following types are supported for the fill_with argument: str, int, float,\n",
      " |              datetime.datetime, and bool.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param fill_with: The value to fill errors with, or None.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  fill_nulls(self, columns:~MultiColumnSelection, fill_with:Any) -> 'Dataflow'\n",
      " |      Fills all nulls in a column with the specified value.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The following types are supported for the fill_with argument: str, int, float,\n",
      " |              datetime.datetime, and bool.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param fill_with: The value to fill nulls with.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  filter(self, expression:azureml.dataprep.api.expressions.Expression) -> 'Dataflow'\n",
      " |      Filters the data, leaving only the records that match the specified expression.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          Expressions are started by indexing the Dataflow with the name of a column. They support a variety of\n",
      " |              functions and operators and can be combined using logical operators. The resulting expression will be\n",
      " |              lazily evaluated for each record when a data pull occurs and not where it is defined.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              dataflow['myColumn'] > dataflow['columnToCompareAgainst']\n",
      " |              dataflow['myColumn'].starts_with('prefix')\n",
      " |      \n",
      " |      :param expression: The expression to evaluate.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  fuzzy_group_column(self, source_column:str, new_column_name:str, similarity_threshold:float=0.8, similarity_score_column_name:str=None) -> 'Dataflow'\n",
      " |      Add a column where similar values from the source column are fuzzy-grouped to their canonical form.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          This method will cluster similar values automatically.\n",
      " |          This is useful when working with data gathered from multiple sources or through human input,\n",
      " |          where the same entities are often represented by multiple values due to different spellings, varying capitalizations, or abbreviations.\n",
      " |          For example, you may have a column with values Chicago, CHICAGO, SEATTLE, Seattle, St. Louis, St. Lois, ST LOUIS, and you want to redeuce them to the distinct values - Chicago, Seattle and St. Louis.\n",
      " |          If you need more control of grouping, create FuzzyGroupBuilder instead.\n",
      " |      \n",
      " |      :param source_column: Column with values to group.\n",
      " |      :param new_column_name: Name of the new column to add.\n",
      " |      :param similarity_threshold: Similarity between values to be grouped together.\n",
      " |      :param similarity_score_column_name: If provided, this transform will also add a column with calculated\n",
      " |          similarity score between every pair of original and canonical values. This information is helpful to determine `similarity_threshold`.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  get_missing_secrets(self) -> List[str]\n",
      " |      Get a list of missing secret IDs.\n",
      " |      \n",
      " |      :return: A list of missing secret IDs.\n",
      " |  \n",
      " |  get_partition_count(self) -> int\n",
      " |      Calculates the partitions for the current Dataflow and returns their count. Partitioning is guaranteed to be stable for a specific execution mode.\n",
      " |      \n",
      " |      :return: The count of partitions.\n",
      " |  \n",
      " |  get_profile(self, include_stype_counts:bool=False, number_of_histogram_bins:int=10) -> azureml.dataprep.api.dataprofile.DataProfile\n",
      " |      Requests the data profile which collects summary statistics on the full data produced by the Dataflow.\n",
      " |          A data profile can be very useful to understand the input data, identify anomalies and missing values,\n",
      " |          and verify that data preparation operations produced the desired result.\n",
      " |      \n",
      " |      :param include_stype_counts: Whether to include checking if values look like some well known semantic types of\n",
      " |          information. For Example, \"email address\". Turning this on will impact performance.\n",
      " |      :paramtype boolean\n",
      " |      :param number_of_histogram_bins: Number of bins in a histogram. If not specified will be set to 10.\n",
      " |      :paramtype int\n",
      " |      :return: DataProfile object\n",
      " |      :rtype: DataProfile\n",
      " |  \n",
      " |  head(self, count:int=10) -> 'pandas.DataFrame'\n",
      " |      Pulls the number of records specified from the top of this Dataflow and returns them as a `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_.\n",
      " |      \n",
      " |      :param count: The number of records to pull. 10 is default.\n",
      " |      :return: A Pandas `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_.\n",
      " |  \n",
      " |  keep_columns(self, columns:~MultiColumnSelection, validate_column_exists:bool=False) -> 'Dataflow'\n",
      " |      Keeps the specified columns and drops all others.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param validate_column_exists: Whether to validate the columns selected.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  label_encode(self, source_column:str, new_column_name:str) -> 'Dataflow'\n",
      " |      Adds a column with encoded labels generated from the source column. For explicit label encoding,\n",
      " |          use LabelEncoderBuilder.\n",
      " |      \n",
      " |      :param source_column: Column name from which encoded labels will be generated.\n",
      " |      :param new_column_name: The new column's name.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  map_column(self, column:str, new_column_id:str, replacements:Union[List[azureml.dataprep.api.dataflow.ReplacementsValue], NoneType]=None) -> 'Dataflow'\n",
      " |      Creates a new column where matching values in the source column have been replaced with the specified values.\n",
      " |      \n",
      " |      :param column: The source column.\n",
      " |      :param new_column_id: The name of the mapped column.\n",
      " |      :param replacements: The values to replace and their replacements.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  map_partition(self, fn:Callable[[Union[_ForwardRef('pandas.DataFrame'), _ForwardRef('scipy.sparse.csr_matrix')], int], Union[_ForwardRef('pandas.DataFrame'), _ForwardRef('scipy.sparse.csr_matrix')]], data_format:str='lastProcessed') -> 'Dataflow'\n",
      " |      Applies a transformation to each partition in the Dataflow.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |          The function passed in must take in two parameters: a `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_ or\n",
      " |              `Link scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_ containing the data for the partition and an index.\n",
      " |              The return value must be a `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_ or a\n",
      " |              `Link scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_ containing the transformed data.\n",
      " |              By default the 'lastProcessed' format is passed into `fn`, i.e. if the data coming into map_partitions is Sparse it will be sent as a\n",
      " |              `Link scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_, if it is dense it will\n",
      " |              be sent as a `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_. The desired\n",
      " |              input data format can be set explicitly using the `data_format` parameter.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              `df` does not usually contain all of the data in the dataflow, but a partition of the data as it is being processed in the                 runtime. The number and contents of each partition is not guaranteed across runs.\n",
      " |          The transform function can fully edit the passed in dataframe or even create a new one, but must return a\n",
      " |              dataframe. Any libraries that the Python script imports must exist in the environment where the dataflow\n",
      " |              is run.\n",
      " |      \n",
      " |      :param fn: A callable that will be invoked to transform each partition.\n",
      " |      :param dataFormat: A optional string specifying the input format to fn. Supported Formats: 'dataframe', 'csr', 'lastProcessed'.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  min_max_scale(self, column:str, range_min:float=0, range_max:float=1, data_min:float=None, data_max:float=None) -> 'Dataflow'\n",
      " |      Scales the values in the specified column to lie within range_min (default=0) and range_max (default=1).\n",
      " |      \n",
      " |      :param column: The column to scale.\n",
      " |      :param range_min: Desired min of scaled values.\n",
      " |      :param range_max: Desired max of scaled values.\n",
      " |      :param data_min: Min of source column. If not provided, a scan of the data will be performed to find the min.\n",
      " |      :param data_max: Max of source column. If not provided, a scan of the data will be performed to find the max.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  multi_split(self, splits:int, seed:Union[int, NoneType]=None)\n",
      " |      Split a Dataflow into multiple other Dataflows, each containing a random but exclusive sub-set of the data.\n",
      " |      \n",
      " |      :param splits: The number of splits.\n",
      " |      :param seed: The seed to use for the random split.\n",
      " |      :return: A list containing one Dataflow per split.\n",
      " |  \n",
      " |  new_script_column(self, new_column_name:str, insert_after:str, script:str) -> 'Dataflow'\n",
      " |      Adds a new column to the Dataflow using the passed in Python script.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The Python script must define a function called newvalue that takes a single argument, typically\n",
      " |              called row. The row argument is a dict (key is column name, value is current value) and will be passed\n",
      " |              to this function for each row in the dataset. This function must return a value to be used in the new column.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              Any libraries that the Python script imports must exist in the environment where the dataflow is run.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import numpy as np\n",
      " |              def newvalue(row):\n",
      " |                  return np.log(row['Metric'])\n",
      " |      \n",
      " |      :param new_column_name: The name of the new column being created.\n",
      " |      :param insert_after: The column after which the new column will be inserted.\n",
      " |      :param script: The script that will be used to create this new column.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  new_script_filter(self, script:str) -> 'Dataflow'\n",
      " |      Filters the Dataflow using the passed in Python script.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The Python script must define a function called includerow that takes a single argument, typically\n",
      " |              called row. The row argument is a dict (key is column name, value is current value) and will be passed\n",
      " |              to this function for each row in the dataset. This function must return True or False depending on whether\n",
      " |              the row should be included in the dataflow. Any libraries that the Python script imports must exist in the\n",
      " |              environment where the dataflow is run.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              def includerow(row):\n",
      " |                  return row['Metric'] > 100\n",
      " |      \n",
      " |      :param script: The script that will be used to filter the dataflow.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  null_coalesce(self, columns:List[str], new_column_id:str) -> 'Dataflow'\n",
      " |      For each record, selects the first non-null value from the columns specified and uses it as the value of a new column.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param new_column_id: The name of the new column.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  one_hot_encode(self, source_column:str, prefix:str=None) -> 'Dataflow'\n",
      " |      Adds a binary column for each categorical label from the source column values. For more control over categorical\n",
      " |          labels, use OneHotEncodingBuilder.\n",
      " |      \n",
      " |      :param source_column: Column name from which categorical labels will be generated.\n",
      " |      :param prefix: An optional prefix to use for the new columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  parse_delimited(self, separator:str, headers_mode:azureml.dataprep.api.dataflow.PromoteHeadersMode, encoding:azureml.dataprep.api.engineapi.typedefinitions.FileEncoding, quoting:bool, skip_rows:int, skip_mode:azureml.dataprep.api.dataflow.SkipMode, comment:str, partition_size:Union[int, NoneType]=None) -> 'Dataflow'\n",
      " |      Adds step to parse CSV data.\n",
      " |      \n",
      " |      :param separator: The separator to use to split columns.\n",
      " |      :param headers_mode: How to determine column headers.\n",
      " |      :param encoding: The encoding of the files being read.\n",
      " |      :param quoting: Whether to handle new line characters within quotes. This option will impact performance.\n",
      " |      :param skip_rows: How many rows to skip.\n",
      " |      :param skip_mode: The mode in which rows are skipped.\n",
      " |      :param comment: Character used to indicate a line is a comment instead of data in the files being read.\n",
      " |      :param partition_size: Desired partition size.\n",
      " |      :return: A new Dataflow with Parse Delimited Step added.\n",
      " |  \n",
      " |  parse_fwf(self, offsets:List[int], headers_mode:azureml.dataprep.api.dataflow.PromoteHeadersMode, encoding:azureml.dataprep.api.engineapi.typedefinitions.FileEncoding, skip_rows:int, skip_mode:azureml.dataprep.api.dataflow.SkipMode) -> 'Dataflow'\n",
      " |      Adds step to parse fixed-width data.\n",
      " |      \n",
      " |      :param offsets: The offsets at which to split columns. The first column is always assumed to start at offset 0.\n",
      " |      :param headers_mode: How to determine column headers.\n",
      " |      :param encoding: The encoding of the files being read.\n",
      " |      :param skip_rows: How many rows to skip.\n",
      " |      :param skip_mode: The mode in which rows are skipped.\n",
      " |      :return: A new Dataflow with Parse FixedWidthFile Step added.\n",
      " |  \n",
      " |  parse_json_column(self, column:str) -> 'Dataflow'\n",
      " |      Parses the values in the specified column as JSON objects and expands them into multiple columns.\n",
      " |      \n",
      " |      :param column: The source column.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  parse_json_lines(self, encoding:azureml.dataprep.api.engineapi.typedefinitions.FileEncoding, partition_size:Union[int, NoneType]=None) -> 'Dataflow'\n",
      " |      Creates a new Dataflow with the operations required to read JSON lines files.\n",
      " |      \n",
      " |      :param encoding: The encoding of the files being read.\n",
      " |      :param partition_size: Desired partition size.\n",
      " |      :return: A new Dataflow with Read JSON line Step added.\n",
      " |  \n",
      " |  parse_lines(self, headers_mode:azureml.dataprep.api.dataflow.PromoteHeadersMode, encoding:azureml.dataprep.api.engineapi.typedefinitions.FileEncoding, skip_rows:int, skip_mode:azureml.dataprep.api.dataflow.SkipMode, comment:str, partition_size:Union[int, NoneType]=None) -> 'Dataflow'\n",
      " |      Adds step to parse text files and split them into lines.\n",
      " |      \n",
      " |      :param headers_mode: How to determine column headers.\n",
      " |      :param encoding: The encoding of the files being read.\n",
      " |      :param skip_rows: How many rows to skip.\n",
      " |      :param skip_mode: The mode in which rows are skipped.\n",
      " |      :param comment: Character used to indicate a line is a comment instead of data in the files being read.\n",
      " |      :param partition_size: Desired partition size.\n",
      " |      :return: A new Dataflow with Parse Lines Step added.\n",
      " |  \n",
      " |  pivot(self, columns_to_pivot:List[str], value_column:str, summary_function:azureml.dataprep.api.dataflow.SummaryFunction=None, group_by_columns:List[str]=None, null_value_replacement:str=None, error_value_replacement:str=None) -> 'Dataflow'\n",
      " |      Returns a new Dataflow with columns generated from the values in the selected columns to pivot.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The values of the new dataflow come from the value column selected.\n",
      " |          Additionally there is an optional summarization that consists of an aggregation and a group by.\n",
      " |      \n",
      " |      :param columns_to_pivot: The columns used to get the values from which the new dataflow's new columns are generated.\n",
      " |      :param value_column: The column used to get the values that will populate the new dataflow's rows.\n",
      " |      :summary_function: The summary function used to aggregate the values.\n",
      " |      :group_by_columns: The columns used to group the new dataflow rows.\n",
      " |      :null_value_replacement: String value to replace null values in columns_to_pivot. If unspecified, the string \"NULL\" will be used.\n",
      " |      :error_value_replacement: String value to replace error values in columns_to_pivot. If unspecified, the string \"ERROR\" will be used.\n",
      " |  \n",
      " |  promote_headers(self) -> 'Dataflow'\n",
      " |      Sets the first record in the dataset as headers, replacing any existing ones.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  quantile_transform(self, source_column:str, new_column:str, quantiles_count:int=1000, output_distribution:str='Uniform')\n",
      " |      Perform quantile transformation to the source_column and output the transformed result in new_column.\n",
      " |      \n",
      " |      :param source_column: The column which quantile transform will be applied to.\n",
      " |      :param new_column: The column where the transformed data will be placed.\n",
      " |      :param quantiles_count: The number of quantiles used. This will be used to discretize the cdf, defaults to 1000\n",
      " |      :param output_distribution: The distribution of the transformed data, defaults to 'Uniform'\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  random_split(self, percentage:float, seed:Union[int, NoneType]=None) -> ('Dataflow', 'Dataflow')\n",
      " |      Returns two Dataflows from the original Dataflow, with records randomly and approximately split by the percentage\n",
      " |          specified (using a seed). If the percentage specified is p (where p is between 0 and 1), the first returned dataflow\n",
      " |          will contain approximately p*100% records from the original dataflow, and the second dataflow will contain all the\n",
      " |          remaining records that were not included in the first. A random seed will be used if none is provided.\n",
      " |      \n",
      " |      :param percentage: The approximate percentage to split the Dataflow by. This must be a number between 0.0 and 1.0.\n",
      " |      :param seed: The seed to use for the random split.\n",
      " |      :return: Two Dataflows containing records randomly split from the original Dataflow. If the percentage specified is p,\n",
      " |          the first dataflow contains approximately p*100% of the records from the original dataflow.\n",
      " |  \n",
      " |  read_excel(self, sheet_name:Union[str, NoneType]=None, use_column_headers:bool=False, skip_rows:int=0) -> 'Dataflow'\n",
      " |      Adds step to read and parse Excel files.\n",
      " |      \n",
      " |      :param sheet_name: The name of the sheet to load. The first sheet is loaded if none is provided.\n",
      " |      :param use_column_headers: Whether to use the first row as column headers.\n",
      " |      :param skip_rows: Number of rows to skip when loading the data.\n",
      " |      :return: A new Dataflow with Read Excel Step added.\n",
      " |  \n",
      " |  read_json(self, json_extract_program:str, encoding:azureml.dataprep.api.engineapi.typedefinitions.FileEncoding)\n",
      " |      Creates a new Dataflow with the operations required to read JSON files.\n",
      " |      \n",
      " |      :param json_extract_program: PROSE program that will be used to parse JSON.\n",
      " |      :param encoding: The encoding of the files being read.\n",
      " |      :return: A new Dataflow with Read JSON Step added.\n",
      " |  \n",
      " |  read_npz_file(self) -> 'Dataflow'\n",
      " |      Adds step to parse npz files.\n",
      " |      \n",
      " |      :return: A new Dataflow with Read Npz File Step added.\n",
      " |  \n",
      " |  read_parquet_file(self) -> 'Dataflow'\n",
      " |      Adds step to parse Parquet files.\n",
      " |      \n",
      " |      :return: A new Dataflow with Parse Parquet File Step added.\n",
      " |  \n",
      " |  read_postgresql(self, data_source:azureml.dataprep.api.datasources.PostgreSQLDataSource, query:str) -> 'Dataflow'\n",
      " |      Adds step that can read data from a PostgreSQL database by executing the query specified.\n",
      " |      \n",
      " |      :param data_source: The details of the PostgreSQL database.\n",
      " |      :param query: The query to execute to read data.\n",
      " |      :return: A new Dataflow with Read SQL Step added.\n",
      " |  \n",
      " |  read_sql(self, data_source:azureml.dataprep.api.datasources.MSSQLDataSource, query:str) -> 'Dataflow'\n",
      " |      Adds step that can read data from an MS SQL database by executing the query specified.\n",
      " |      \n",
      " |      :param data_source: The details of the MS SQL database.\n",
      " |      :param query: The query to execute to read data.\n",
      " |      :return: A new Dataflow with Read SQL Step added.\n",
      " |  \n",
      " |  rename_columns(self, column_pairs:Dict[str, str]) -> 'Dataflow'\n",
      " |      Renames the specified columns.\n",
      " |      \n",
      " |      :param column_pairs: The columns to rename and the desired new names.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  replace(self, columns:~MultiColumnSelection, find:Any, replace_with:Any) -> 'Dataflow'\n",
      " |      Replaces values in a column that match the specified search value.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The following types are supported for both the find or replace arguments: str, int, float,\n",
      " |              datetime.datetime, and bool.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param find: The value to find, or None.\n",
      " |      :param replace_with: The replacement value, or None.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  replace_datasource(self, new_datasource:~DataSource) -> 'Dataflow'\n",
      " |      Returns new Dataflow with its DataSource replaced by the given one.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The given 'new_datasource' must match the type of datasource already in the Dataflow.\n",
      " |          For example a MSSQLDataSource cannot be replaced with a FileDataSource.\n",
      " |      \n",
      " |      :param new_datasource: DataSource to substitute into new Dataflow.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  replace_na(self, columns:~MultiColumnSelection, use_default_na_list:bool=True, use_empty_string_as_na:bool=True, use_nan_as_na:bool=True, custom_na_list:Union[str, NoneType]=None) -> 'Dataflow'\n",
      " |      Replaces values in the specified columns with nulls. You can choose to use the default list, supply your own, or both.\n",
      " |      \n",
      " |      :param use_default_na_list: Use the default list and replace 'null', 'NaN', 'NA', and 'N/A' with null.\n",
      " |      :param use_empty_string_as_na: Replace empty strings with null.\n",
      " |      :param use_nan_as_na: Replace NaNs with Null.\n",
      " |      :param custom_na_list: Provide a comma separated list of values to replace with null.\n",
      " |      :param columns: The source columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  replace_reference(self, new_reference:~DataflowReference) -> 'Dataflow'\n",
      " |      Returns new Dataflow with its reference DataSource replaced by the given one.\n",
      " |      \n",
      " |      :param new_reference: Reference to be substituted for current Reference in Dataflow.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  round(self, column:str, decimal_places:int) -> 'Dataflow'\n",
      " |      Rounds the values in the column specified to the desired number of decimal places.\n",
      " |      \n",
      " |      :param column: The source column.\n",
      " |      :param decimal_places: The number of decimal places.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  run_local(self) -> None\n",
      " |      Runs the current Dataflow using the local execution runtime.\n",
      " |  \n",
      " |  run_spark(self) -> None\n",
      " |      Runs the current Dataflow using the Spark runtime.\n",
      " |  \n",
      " |  save(self, file_path:str)\n",
      " |      Saves the Dataflow to the specified file\n",
      " |      \n",
      " |      :param file_path: The path of the file.\n",
      " |  \n",
      " |  select_partitions(self, partition_indices:List[int]) -> 'Dataflow'\n",
      " |      Selects specific partitions from the data, dropping the rest.\n",
      " |      \n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  set_column_types(self, type_conversions:Dict[str, ~TypeConversionInfo]) -> 'Dataflow'\n",
      " |      Converts values in specified columns to the corresponding data types.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The values in the type_conversions dictionary could be of several types:\n",
      " |      \n",
      " |          * :class:`azureml.dataprep.FieldType`\n",
      " |          * :class:`azureml.dataprep.TypeConverter`\n",
      " |          * Tuple of :class:`FieldType.DATE <azureml.dataprep.FieldType>` and List of format strings (single format string is also supported)\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import azureml.dataprep as dprep\n",
      " |      \n",
      " |              dataflow = dprep.read_csv(path='./some/path')\n",
      " |              dataflow = dataflow.set_column_types({'MyNumericColumn': dprep.FieldType.DECIMAL,\n",
      " |                                                 'MyBoolColumn': dprep.FieldType.BOOLEAN,\n",
      " |                                                 'MyAutodetectDateColumn': dprep.FieldType.DATE,\n",
      " |                                                 'MyDateColumnWithFormat': (dprep.FieldType.DATE, ['%m-%d-%Y']),\n",
      " |                                                 'MyOtherDateColumn': dprep.DateTimeConverter(['%d-%m-%Y'])})\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If you choose to convert a column to :class:`FieldType.DATE <azureml.dataprep.FieldType>` and do not provide                 **format(s)** to use, DataPrep will attempt to infer a format to use by pulling on the data.                 If a format could be inferred from the data, it will be used to convert values in the corresponding\n",
      " |              column. However, if a format could not be inferred, this step will fail and you will need to either                 provide the format to use or use the interactive builder                 :class:`azureml.dataprep.api.builders.ColumnTypesBuilder` by calling                 'dataflow.builders.set_column_types()'\n",
      " |      \n",
      " |      :param type_conversions: A dictionary where key is column name and value is either\n",
      " |          :class:`azureml.dataprep.FieldType` or :class:`azureml.dataprep.TypeConverter` or a Tuple of\n",
      " |          :class:`FieldType.DATE <azureml.dataprep.FieldType>` and List of format strings\n",
      " |      \n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  skip(self, count:int) -> 'Dataflow'\n",
      " |      Skips the specified number of records.\n",
      " |      \n",
      " |      :param count: The number of records to skip.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  sort(self, sort_order:List[Tuple[str, bool]]) -> 'Dataflow'\n",
      " |      Sorts the dataset by the specified columns.\n",
      " |      \n",
      " |      :param sort_order: The columns to sort by and whether to sort ascending or descending. True is treated as descending, false as ascending.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  sort_asc(self, columns:~SortColumns) -> 'Dataflow'\n",
      " |      Sorts the dataset in ascending order by the specified columns.\n",
      " |      \n",
      " |      :param columns: The columns to sort in ascending order.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  sort_desc(self, columns:~SortColumns) -> 'Dataflow'\n",
      " |      Sorts the dataset in descending order by the specified columns.\n",
      " |      \n",
      " |      :param columns: The columns to sort in descending order.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  split_column_by_delimiters(self, source_column:str, delimiters:~Delimiters, keep_delimiters:False) -> 'Dataflow'\n",
      " |      Splits the provided column and adds the resulting columns to the dataflow.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          This will pull small sample of the data, determine number of columns it should expect as a result of the\n",
      " |              split and generate a split program that would ensure that the expected number of columns will be produced,\n",
      " |              so that there is a deterministic schema after this operation.\n",
      " |      \n",
      " |      :param source_column: Column to split.\n",
      " |      :param delimiters: String or list of strings to be deemed as column delimiters.\n",
      " |      :param keep_delimiters: Controls whether to keep or drop column with delimiters.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  split_column_by_example(self, source_column:str, example:~SplitExample=None) -> 'Dataflow'\n",
      " |      Splits the provided column and adds the resulting columns to the dataflow based on the provided example.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          This will pull small sample of the data, determine the best program to satisfy provided example\n",
      " |              and generate a split program that would ensure that the expected number of columns will be produced, so that\n",
      " |              there is a deterministic schema after this operation.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              If example is not provided, this will generate a split program based on common split patterns, like splitting by space, punctuation, date parts and etc.\n",
      " |      \n",
      " |      :param source_column: Column to split.\n",
      " |      :param example: Example to use for program generation.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  split_stype(self, column:str, stype:azureml.dataprep.api.dataflow.SType, stype_fields:Union[List[str], NoneType]=None, new_column_names:Union[List[str], NoneType]=None) -> 'Dataflow'\n",
      " |      Creates new columns from an existing column, interpreting its values as a semantic type.\n",
      " |      \n",
      " |      :param column: The source column.\n",
      " |      :param stype: The semantic type used to interpret values in the column.\n",
      " |      :param stype_fields: Fields of the semantic type to use. If not provided, all fields will be used.\n",
      " |      :param new_column_names: Names of the new columns. If not provided new columns will be named with the source column name plus the semantic type field name.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  str_replace(self, columns:~MultiColumnSelection, value_to_find:Union[str, NoneType]=None, replace_with:Union[str, NoneType]=None, match_entire_cell_contents:bool=False, use_special_characters:bool=False) -> 'Dataflow'\n",
      " |      Replaces values in a string column that match a search string with the specified value.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param value_to_find: The value to find.\n",
      " |      :param replace_with: The replacement value.\n",
      " |      :param match_entire_cell_contents: Whether the value to find must match the entire value.\n",
      " |      :param use_special_characters: If checked, you can use '#(tab)', '#(cr)', or '#(lf)' to represent special characters in the find or replace arguments.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  summarize(self, summary_columns:Union[List[azureml.dataprep.api.dataflow._SummaryColumnsValue], NoneType]=None, group_by_columns:Union[List[str], NoneType]=None, join_back:bool=False, join_back_columns_prefix:Union[str, NoneType]=None) -> 'Dataflow'\n",
      " |      Summarizes data by running aggregate functions over specific columns.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The aggregate functions are independent and it is possible to aggregate the same column multiple times.\n",
      " |              Unique names have to be provided for the resulting columns. The aggregations can be grouped, in which case\n",
      " |              one record is returned per group; or ungrouped, in which case one record is returned for the whole dataset.\n",
      " |              Additionally, the results of the aggregations can either replace the current dataset or augment it by\n",
      " |              appending the result columns.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import azureml.dataprep as dprep\n",
      " |      \n",
      " |              dataflow = dprep.read_csv(path='./some/path')\n",
      " |              dataflow = dataflow.summarize(\n",
      " |                  summary_columns=[\n",
      " |                      dprep.SummaryColumnsValue(\n",
      " |                          column_id='Column To Summarize',\n",
      " |                          summary_column_name='New Column of Counts',\n",
      " |                          summary_function=dprep.SummaryFunction.COUNT)],\n",
      " |                  group_by_columns=['Column 1', 'Column 2'])\n",
      " |      \n",
      " |      :param summary_columns: List of SummaryColumnsValue, where each value defines the column to summarize,\n",
      " |          the summary function to use and the name of resulting column to add.\n",
      " |      :param group_by_columns: Columns to group by.\n",
      " |      :param join_back: Whether to append subtotals or replace current data with them.\n",
      " |      :param join_back_columns_prefix: Prefix to use for subtotal columns when appending them to current data.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  take(self, count:int) -> 'Dataflow'\n",
      " |      Takes the specified count of records.\n",
      " |      \n",
      " |      :param count: The number of records to take.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  take_sample(self, probability:float, seed:Union[int, NoneType]=None) -> 'Dataflow'\n",
      " |      Takes a random sample of the available records.\n",
      " |      \n",
      " |      :param probability: The probability of a record being included in the sample.\n",
      " |      :param seed: The seed to use for the random generator.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  take_stratified_sample(self, columns:~MultiColumnSelection, fractions:Dict[Tuple, int], seed:Union[int, NoneType]=None) -> 'Dataflow'\n",
      " |      Takes a random stratified sample of the available records according to input fractions.\n",
      " |      \n",
      " |      :param columns: The strata columns.\n",
      " |      :param fractions: The strata to strata weights.\n",
      " |      :param seed: The seed to use for the random generator.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_bool(self, columns:~MultiColumnSelection, true_values:List[str], false_values:List[str], mismatch_as:azureml.dataprep.api.dataflow.MismatchAsOption=<MismatchAsOption.ASERROR: 2>) -> 'Dataflow'\n",
      " |      Converts the values in the specified columns to booleans.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param true_values: The values to treat as true.\n",
      " |      :param false_values: The values to treat as false.\n",
      " |      :param mismatch_as: How to treat values that don't match the values in the true or false values lists.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_csv_streams(self, separator:str=',', na:str='NA', error:str='ERROR') -> 'Dataflow'\n",
      " |      Creates streams with the data in delimited format.\n",
      " |      \n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_data_frame_directory(self, error:str='ERROR', rows_per_group:int=5000) -> 'Dataflow'\n",
      " |      Creates streams with the data in dataframe directory format.\n",
      " |      \n",
      " |      :param error: String to use for error values.\n",
      " |      :param rows_per_group: Number of rows to use per row group.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_datetime(self, columns:~MultiColumnSelection, date_time_formats:Union[List[str], NoneType]=None, date_constant:Union[str, NoneType]=None) -> 'Dataflow'\n",
      " |      Converts the values in the specified columns to DateTimes.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param date_time_formats: The formats to use to parse the values. If none are provided, a partial scan of the\n",
      " |          data will be performed to derive one.\n",
      " |      :param date_constant: If the column contains only time values, a date to apply to the resulting DateTime.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_json(self) -> str\n",
      " |      Get the JSON string representation of the Dataflow.\n",
      " |  \n",
      " |  to_long(self, columns:~MultiColumnSelection) -> 'Dataflow'\n",
      " |      Converts the values in the specified columns to 64 bit integers.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_number(self, columns:~MultiColumnSelection, decimal_point:azureml.dataprep.api.dataflow.DecimalMark=<DecimalMark.DOT: 0>) -> 'Dataflow'\n",
      " |      Converts the values in the specified columns to floating point numbers.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param decimal_point: The symbol to use as the decimal mark.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_pandas_dataframe(self, extended_types:bool=False, nulls_as_nan:bool=True) -> 'pandas.DataFrame'\n",
      " |      Pulls all of the data and returns it as a Pandas `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          This method will load all the data returned by this Dataflow into memory.\n",
      " |      \n",
      " |          Since Dataflows do not require a fixed, tabular schema but Pandas DataFrames do, an implicit tabularization\n",
      " |              step will be executed as part of this action. The resulting schema will be the union of the schemas of all\n",
      " |              records produced by this Dataflow.\n",
      " |      \n",
      " |      :param extended_types: Whether to keep extended DataPrep types such as DataPrepError in the DataFrame. If False,\n",
      " |          these values will be replaced with None.\n",
      " |      :param nulls_as_nan: Whether to interpret nulls (or missing values) in number typed columns as NaN. This is\n",
      " |          done by pandas for performance reasons; it can result in a loss of fidelity in the data.\n",
      " |      :return: A Pandas `Link pandas.DataFrame <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html>`_.\n",
      " |  \n",
      " |  to_parquet_streams(self, error:str='ERROR', rows_per_group:int=5000) -> 'Dataflow'\n",
      " |      Creates streams with the data in parquet format.\n",
      " |      \n",
      " |      :param error: String to use for error values.\n",
      " |      :param rows_per_group: Number of rows to use per row group.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  to_partition_iterator(self) -> azureml.dataprep.api._partitionsreader.PartitionIterable\n",
      " |      Creates an iterable object that returns the partitions produced by this Dataflow in sequence. This iterable\n",
      " |      must be closed before any other executions can run.\n",
      " |      \n",
      " |      :return: A PartitionsIterable object that can be used to iterate over the partitions in this Dataflow.\n",
      " |  \n",
      " |  to_record_iterator(self) -> azureml.dataprep.api._dataframereader.RecordIterable\n",
      " |      Creates an iterable object that returns the records produced by this Dataflow in sequence. This iterable\n",
      " |      must be closed before any other executions can run.\n",
      " |      \n",
      " |      :return: A RecordIterable object that can be used to iterate over the records in this Dataflow.\n",
      " |  \n",
      " |  to_spark_dataframe(self) -> 'pyspark.sql.DataFrame'\n",
      " |      Creates a Spark `Link pyspark.sql.DataFrame <https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame>`_ that can execute the transformation pipeline defined by this Dataflow.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          Since Dataflows do not require a fixed, tabular schema but Spark Dataframes do, an implicit tabularization\n",
      " |              step will be executed as part of this action. The resulting schema will be the union of the schemas of all\n",
      " |              records produced by this Dataflow. This tabularization step will result in a pull of the data.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              The Spark Dataframe returned is only an execution plan and doesn't actually contain any data, since Spark Dataframes are also lazily evaluated.\n",
      " |      \n",
      " |      :return: A Spark `Link pyspark.sql.DataFrame <https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame>`_.\n",
      " |  \n",
      " |  to_string(self, columns:~MultiColumnSelection) -> 'Dataflow'\n",
      " |      Converts the values in the specified columns to strings.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  transform_partition(self, script:'str') -> 'Dataflow'\n",
      " |      Applies a transformation to each partition in the Dataflow.\n",
      " |      \n",
      " |      * This function has been deprecated and will be removed in a future version. Please use `map_partition` instead.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The Python script must define a function called transform that takes two arguments, typically called `df` and\n",
      " |              `index`. The `df` argument will be a Pandas Dataframe passed to this function that contains the data for the\n",
      " |              partition and the `index` argument is a unique identifier of the partition.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              `df` does not usually contain all of the data in the dataflow, but a partition of the data as it is being processed in the                 runtime. The number and contents of each partition is not guaranteed across runs.\n",
      " |          The transform function can fully edit the passed in dataframe or even create a new one, but must return a\n",
      " |              dataframe. Any libraries that the Python script imports must exist in the environment where the dataflow\n",
      " |              is run.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # the script passed in should have this function defined\n",
      " |              def transform(df, index):\n",
      " |                  # perform any partition level transforms here and return resulting `df`\n",
      " |                  return df\n",
      " |      \n",
      " |      :param script: A script that will be used to transform each partition.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  transform_partition_with_file(self, script_path:str) -> 'Dataflow'\n",
      " |      Transforms an entire partition using the Python script in the passed in file.\n",
      " |      \n",
      " |      .. remarks::\n",
      " |      \n",
      " |          The Python script must define a function called transform that takes two arguments, typically called `df` and\n",
      " |              `index`. The first argument `df` will be a Pandas Dataframe that contains the data for the partition and the\n",
      " |              second argument `index` will be a unique identifier for the partition.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              `df` does not usually contain all of the data in the dataflow, but a partition of the data as it is being processed in the runtime.                The number and contents of each partition is not guaranteed across runs.\n",
      " |      \n",
      " |          The transform function can fully edit the passed in dataframe or even create a new one, but must return a\n",
      " |              dataframe. Any libraries that the Python script imports must exist in the environment where the dataflow is run.\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # the script file passed in should have this function defined\n",
      " |              def transform(df, index):\n",
      " |                  # perform any partition level transforms here and return resulting `df`\n",
      " |                  return df\n",
      " |      \n",
      " |      :param script_path: Relative path to script that will be used to transform the partition.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  trim_string(self, columns:~MultiColumnSelection, trim_left:bool=True, trim_right:bool=True, trim_type:azureml.dataprep.api.dataflow.TrimType=<TrimType.WHITESPACE: 0>, custom_characters:str='') -> 'Dataflow'\n",
      " |      Trims string values in specific columns.\n",
      " |      \n",
      " |      :param columns: The source columns.\n",
      " |      :param trim_left: Whether to trim from the beginning.\n",
      " |      :param trim_right: Whether to trim from the end.\n",
      " |      :param trim_type: Whether to trim whitespace or custom characters.\n",
      " |      :param custom_characters: The characters to trim.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  use_secrets(self, secrets:Dict[str, str])\n",
      " |      Uses the passed in secrets for execution.\n",
      " |      \n",
      " |      :param secrets: A dictionary of secret ID to secret value. You can get the list of required secrets by calling\n",
      " |          the get_missing_secrets method on Dataflow.\n",
      " |  \n",
      " |  verify_has_data(self)\n",
      " |      Verifies that this Dataflow would produce records if executed. An exception will be thrown otherwise.\n",
      " |  \n",
      " |  write_streams(self, streams_column:str, base_path:azureml.dataprep.api.datasources.FileOutput, file_names_column:Union[str, NoneType]=None) -> 'Dataflow'\n",
      " |      Writes the streams in the specified column to the destination path. By default, the name of the files written will be the resource identifier\n",
      " |          of the streams. This behavior can be overriden by specifying a column which contains the names to use.\n",
      " |      \n",
      " |      :param streams_column: The column containing the streams to write.\n",
      " |      :param file_names_column: A column containing the file names to use.\n",
      " |      :param base_path: The path under which the files should be written.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  write_to_csv(self, directory_path:~DataDestination, separator:str=',', na:str='NA', error:str='ERROR') -> 'Dataflow'\n",
      " |      Write out the data in the Dataflow in a delimited text format. The output is specified as a directory\n",
      " |          which will contain multiple files, one per partition processed in the Dataflow.\n",
      " |      \n",
      " |      :param directory_path: The path to a directory in which to store the output files.\n",
      " |      :param separator: The separator to use.\n",
      " |      :param na: String to use for null values.\n",
      " |      :param error: String to use for error values.\n",
      " |      :return: The modified Dataflow. Every execution of the returned Dataflow will perform the write again.\n",
      " |  \n",
      " |  write_to_parquet(self, file_path:Union[~DataDestination, NoneType]=None, directory_path:Union[~DataDestination, NoneType]=None, single_file:bool=False, error:str='ERROR', row_groups:int=0) -> 'Dataflow'\n",
      " |      Writes out the data in the Dataflow into Parquet files.\n",
      " |      \n",
      " |      :param file_path: The path in which to store the output file.\n",
      " |      :param directory_path: The path in which to store the output files.\n",
      " |      :param single_file: Whether to store the entire Dataflow in a single file.\n",
      " |      :param error: String to use for error values.\n",
      " |      :param row_groups: Number of rows to use per row group.\n",
      " |      :return: The modified Dataflow.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_json(dataflow_json:str) -> 'Dataflow'\n",
      " |      Load Dataflow from 'package_json'.\n",
      " |      \n",
      " |      :param dataflow_json: JSON string representation of the Package.\n",
      " |      :return: New Package object constructed from the JSON string.\n",
      " |  \n",
      " |  get_files(path:~FilePath) -> 'Dataflow'\n",
      " |      Expands the path specified by reading globs and files in folders and outputs one record per file found.\n",
      " |      \n",
      " |      :param path: The path or paths to expand.\n",
      " |      :return: A new Dataflow.\n",
      " |  \n",
      " |  join(left_dataflow:~DataflowReference, right_dataflow:~DataflowReference, join_key_pairs:List[Tuple[str, str]]=None, join_type:azureml.dataprep.api.dataflow.JoinType=<JoinType.MATCH: 2>, left_column_prefix:str='l_', right_column_prefix:str='r_', left_non_prefixed_columns:List[str]=None, right_non_prefixed_columns:List[str]=None) -> 'Dataflow'\n",
      " |      Creates a new Dataflow that is a result of joining two provided Dataflows.\n",
      " |      \n",
      " |      :param left_dataflow: Left Dataflow or DataflowReference to join with.\n",
      " |      :param right_dataflow: Right Dataflow or DataflowReference to join with.\n",
      " |      :param join_key_pairs: Key column pairs. List of tuples of columns names where each tuple forms a key pair to\n",
      " |          join on. For instance: [('column_from_left_dataflow', 'column_from_right_dataflow')]\n",
      " |      :param join_type: Type of join to perform. Match is default.\n",
      " |      :param left_column_prefix: Prefix to use in result Dataflow for columns originating from left_dataflow.\n",
      " |          Needed to avoid column name conflicts at runtime.\n",
      " |      :param right_column_prefix: Prefix to use in result Dataflow for columns originating from right_dataflow.\n",
      " |          Needed to avoid column name conflicts at runtime.\n",
      " |      :param left_non_prefixed_columns: List of column names from left_dataflow that should not be prefixed with\n",
      " |          left_column_prefix. Every other column appearing in the data at runtime will be prefixed.\n",
      " |      :param right_non_prefixed_columns: List of column names from right_dataflow that should not be prefixed with\n",
      " |          left_column_prefix. Every other column appearing in the data at runtime will be prefixed.\n",
      " |      :return: The new Dataflow.\n",
      " |  \n",
      " |  open(file_path:str) -> 'Dataflow'\n",
      " |      Opens a Dataflow with specified name from the package file.\n",
      " |      \n",
      " |      :param file_path: Path to the package containing the Dataflow.\n",
      " |      :return: The Dataflow.\n",
      " |  \n",
      " |  read_parquet_dataset(path:azureml.dataprep.api.datasources.FileDataSource) -> 'Dataflow'\n",
      " |      Creates a step to read parquet file.\n",
      " |      \n",
      " |      :param path: The path to the Parquet file.\n",
      " |      :return: A new Dataflow.\n",
      " |  \n",
      " |  reference(reference:'DataflowReference') -> 'Dataflow'\n",
      " |      Creates a reference to an existing activity object.\n",
      " |      \n",
      " |      :param reference: The reference activity.\n",
      " |      :return: A new Dataflow.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  dtypes\n",
      " |      Gets column data types for the current dataset by calling :meth:`get_profile` and extracting just dtypes from\n",
      " |          the resulting DataProfile.\n",
      " |      \n",
      " |      .. note:\n",
      " |      \n",
      " |          This will trigger a data profile calculation.\n",
      " |          To avoid calculating profile multiple times, get the profile first by calling             `get_profile()` and then inspect it.\n",
      " |      \n",
      " |      :return: A dictionary, where key is the column name and value is :class:`azureml.dataprep.FieldType`.\n",
      " |  \n",
      " |  row_count\n",
      " |      Count of rows in this Dataflow.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This will trigger a data profile calculation. To avoid calculating profile multiple times, get the profile first by calling             `get_profile()` and then inspect it.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If current Dataflow contains `take_sample` step or 'take' step, this will return number of rows in the             subset defined by those steps.\n",
      " |      \n",
      " |      :return: Count of rows.\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of the data produced by the Dataflow.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This will trigger a data profile calculation. To avoid calculating profile multiple times, get the profile first by calling             `get_profile()` and then inspect it.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If current Dataflow contains `take_sample` step or 'take' step, this will return number of rows in the             subset defined by those steps.\n",
      " |      \n",
      " |      :return: Tuple of row count and column count.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ExampleData = ~ExampleData\n",
      " |      Type variable.\n",
      " |      \n",
      " |      Usage::\n",
      " |      \n",
      " |        T = TypeVar('T')  # Can be anything\n",
      " |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      " |      \n",
      " |      Type variables exist primarily for the benefit of static type\n",
      " |      checkers.  They serve as the parameters for generic types as well\n",
      " |      as for generic function definitions.  See class Generic for more\n",
      " |      information on generic types.  Generic functions work as follows:\n",
      " |      \n",
      " |        def repeat(x: T, n: int) -> List[T]:\n",
      " |            '''Return a list containing n references to x.'''\n",
      " |            return [x]*n\n",
      " |      \n",
      " |        def longest(x: A, y: A) -> A:\n",
      " |            '''Return the longest of two strings.'''\n",
      " |            return x if len(x) >= len(y) else y\n",
      " |      \n",
      " |      The latter example's signature is essentially the overloading\n",
      " |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      " |      that if the arguments are instances of some subclass of str,\n",
      " |      the return type is still plain str.\n",
      " |      \n",
      " |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      " |      \n",
      " |      Type variables defined with covariant=True or contravariant=True\n",
      " |      can be used do declare covariant or contravariant generic types.\n",
      " |      See PEP 484 for more details. By default generic types are invariant\n",
      " |      in all type variables.\n",
      " |      \n",
      " |      Type variables can be introspected. e.g.:\n",
      " |      \n",
      " |        T.__name__ == 'T'\n",
      " |        T.__constraints__ == ()\n",
      " |        T.__covariant__ == False\n",
      " |        T.__contravariant__ = False\n",
      " |        A.__constraints__ == (str, bytes)\n",
      " |  \n",
      " |  SortColumns = ~SortColumns\n",
      " |      Type variable.\n",
      " |      \n",
      " |      Usage::\n",
      " |      \n",
      " |        T = TypeVar('T')  # Can be anything\n",
      " |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      " |      \n",
      " |      Type variables exist primarily for the benefit of static type\n",
      " |      checkers.  They serve as the parameters for generic types as well\n",
      " |      as for generic function definitions.  See class Generic for more\n",
      " |      information on generic types.  Generic functions work as follows:\n",
      " |      \n",
      " |        def repeat(x: T, n: int) -> List[T]:\n",
      " |            '''Return a list containing n references to x.'''\n",
      " |            return [x]*n\n",
      " |      \n",
      " |        def longest(x: A, y: A) -> A:\n",
      " |            '''Return the longest of two strings.'''\n",
      " |            return x if len(x) >= len(y) else y\n",
      " |      \n",
      " |      The latter example's signature is essentially the overloading\n",
      " |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      " |      that if the arguments are instances of some subclass of str,\n",
      " |      the return type is still plain str.\n",
      " |      \n",
      " |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      " |      \n",
      " |      Type variables defined with covariant=True or contravariant=True\n",
      " |      can be used do declare covariant or contravariant generic types.\n",
      " |      See PEP 484 for more details. By default generic types are invariant\n",
      " |      in all type variables.\n",
      " |      \n",
      " |      Type variables can be introspected. e.g.:\n",
      " |      \n",
      " |        T.__name__ == 'T'\n",
      " |        T.__constraints__ == ()\n",
      " |        T.__covariant__ == False\n",
      " |        T.__contravariant__ = False\n",
      " |        A.__constraints__ == (str, bytes)\n",
      " |  \n",
      " |  SourceColumns = ~SourceColumns\n",
      " |      Type variable.\n",
      " |      \n",
      " |      Usage::\n",
      " |      \n",
      " |        T = TypeVar('T')  # Can be anything\n",
      " |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      " |      \n",
      " |      Type variables exist primarily for the benefit of static type\n",
      " |      checkers.  They serve as the parameters for generic types as well\n",
      " |      as for generic function definitions.  See class Generic for more\n",
      " |      information on generic types.  Generic functions work as follows:\n",
      " |      \n",
      " |        def repeat(x: T, n: int) -> List[T]:\n",
      " |            '''Return a list containing n references to x.'''\n",
      " |            return [x]*n\n",
      " |      \n",
      " |        def longest(x: A, y: A) -> A:\n",
      " |            '''Return the longest of two strings.'''\n",
      " |            return x if len(x) >= len(y) else y\n",
      " |      \n",
      " |      The latter example's signature is essentially the overloading\n",
      " |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      " |      that if the arguments are instances of some subclass of str,\n",
      " |      the return type is still plain str.\n",
      " |      \n",
      " |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      " |      \n",
      " |      Type variables defined with covariant=True or contravariant=True\n",
      " |      can be used do declare covariant or contravariant generic types.\n",
      " |      See PEP 484 for more details. By default generic types are invariant\n",
      " |      in all type variables.\n",
      " |      \n",
      " |      Type variables can be introspected. e.g.:\n",
      " |      \n",
      " |        T.__name__ == 'T'\n",
      " |        T.__constraints__ == ()\n",
      " |        T.__covariant__ == False\n",
      " |        T.__contravariant__ = False\n",
      " |        A.__constraints__ == (str, bytes)\n",
      " |  \n",
      " |  TypeConversionInfo = ~TypeConversionInfo\n",
      " |      Type variable.\n",
      " |      \n",
      " |      Usage::\n",
      " |      \n",
      " |        T = TypeVar('T')  # Can be anything\n",
      " |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      " |      \n",
      " |      Type variables exist primarily for the benefit of static type\n",
      " |      checkers.  They serve as the parameters for generic types as well\n",
      " |      as for generic function definitions.  See class Generic for more\n",
      " |      information on generic types.  Generic functions work as follows:\n",
      " |      \n",
      " |        def repeat(x: T, n: int) -> List[T]:\n",
      " |            '''Return a list containing n references to x.'''\n",
      " |            return [x]*n\n",
      " |      \n",
      " |        def longest(x: A, y: A) -> A:\n",
      " |            '''Return the longest of two strings.'''\n",
      " |            return x if len(x) >= len(y) else y\n",
      " |      \n",
      " |      The latter example's signature is essentially the overloading\n",
      " |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      " |      that if the arguments are instances of some subclass of str,\n",
      " |      the return type is still plain str.\n",
      " |      \n",
      " |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      " |      \n",
      " |      Type variables defined with covariant=True or contravariant=True\n",
      " |      can be used do declare covariant or contravariant generic types.\n",
      " |      See PEP 484 for more details. By default generic types are invariant\n",
      " |      in all type variables.\n",
      " |      \n",
      " |      Type variables can be introspected. e.g.:\n",
      " |      \n",
      " |        T.__name__ == 'T'\n",
      " |        T.__constraints__ == ()\n",
      " |        T.__covariant__ == False\n",
      " |        T.__contravariant__ = False\n",
      " |        A.__constraints__ == (str, bytes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(dset._dataflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws, 'dask')\n",
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = exp.get_runs()\n",
    "run = next(runs)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "c = Client('tcp://localhost:8786')\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = run.get_metrics()['datastore']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ds + ''\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = dd.read_csv(path+'/datasets/isd/*data.csv', dtype={'usaf': 'object'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NoaaIsd().to_dask_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.delayed(load_data)(path).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.datetime = dd.to_datetime(df.datetime).dt.floor('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(df.datetime, sorted=True).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = df.groupby(df.index).mean().compute()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(path):\n",
    "    df.to_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dask.delayed(write_data)(ds+'/dask/outputs/isd').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby([df.index.month, df.index.year]).day.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [counts[month][2015] for month in range(1, 13)]\n",
    "cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2015, 1, 1), datetime(2015, 12, 1)])\n",
    "    plt.grid()\n",
    "    \n",
    "    run.log_image(col, plot=plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.memory_usage(index=True, deep=True).sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the cluster works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from dask import delayed, visualize\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(abs(np.random.normal(5, 2)))\n",
    "    return x + 1\n",
    "\n",
    "fut = []\n",
    "for i in range(10):\n",
    "    fut.append( c.submit(delayed(inc), i) )\n",
    "\n",
    "fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in fut:\n",
    "    print(i.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a):\n",
    "    x = 0\n",
    "    for y in a:\n",
    "        x += y\n",
    "    return x\n",
    "\n",
    "results = []\n",
    "for f in fut:\n",
    "    results.append(f.result())\n",
    "    \n",
    "fut2 = c.submit(sum, results)\n",
    "fut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut2.result().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(fut2.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Large Datasets\n",
    "(from https://github.com/dask/dask-tutorial)\n",
    "\n",
    "Sometimes you'll want to train on a larger than memory dataset. `dask-ml` has implemented estimators that work well on dask arrays and dataframes that may be larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "import dask.array as da\n",
    "import dask.delayed\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a small (random) dataset locally using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_centers = 12\n",
    "n_features = 20\n",
    "\n",
    "X_small, y_small = make_blobs(n_samples=1000, centers=n_centers, n_features=n_features, random_state=0)\n",
    "\n",
    "centers = np.zeros((n_centers, n_features))\n",
    "\n",
    "for i in range(n_centers):\n",
    "    centers[i] = X_small[y_small == i].mean(0)\n",
    "    \n",
    "centers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small dataset will be the template for our large random dataset.\n",
    "We'll use `dask.delayed` to adapt `sklearn.datasets.make_blobs`, so that the actual dataset is being generated on our workers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_block = 200000\n",
    "n_blocks = 500\n",
    "\n",
    "delayeds = [dask.delayed(make_blobs)(n_samples=n_samples_per_block,\n",
    "                                     centers=centers,\n",
    "                                     n_features=n_features,\n",
    "                                     random_state=i)[0]\n",
    "            for i in range(n_blocks)]\n",
    "arrays = [da.from_delayed(obj, shape=(n_samples_per_block, n_features), dtype='float64')\n",
    "          for obj in delayeds]\n",
    "X = da.concatenate(arrays)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the array\n",
    "X.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this on the cluster.\n",
    "X = X.persist()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms implemented in Dask-ML are scalable. They handle larger-than-memory datasets just fine.\n",
    "\n",
    "They follow the scikit-learn API, so if you're familiar with scikit-learn, you'll feel at home with Dask-ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.cluster import KMeans\n",
    "clf = KMeans(init_max_iter=3, oversampling_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_[:10].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shut cluster down\n",
    "To shut the cluster down, cancel the job that runs the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['dask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'cancelling run {run.id}')\n",
    "        run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just for convenience, get the latest running Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in ws.experiments['dask'].get_runs():\n",
    "    if run.get_status() == \"Running\":\n",
    "        print(f'latest running run is {run.id}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
