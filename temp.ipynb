{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception cannot import name '_DistributedTraining'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask\n",
    "import time\n",
    "import joblib\n",
    "import fsspec\n",
    "import socket\n",
    "import matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from dask.distributed import Client\n",
    "from IPython.core.display import HTML\n",
    "from dask_ml.xgboost import XGBRegressor\n",
    "\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment, Model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='uks-azureml', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copeters-rg')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Name: cody100\n",
      "\n",
      "vNET RG: copeters-rg\n",
      "vNET name: uksouth-vnet\n",
      "vNET subnet name: default\n",
      "\n",
      "Environment: cody100-dask-env\n",
      "Compute target: cody100-dask-ct\n",
      "Experiment name: cody100-dask-demo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### name\n",
    "name        = 'cody100'             # REPLACE\n",
    "\n",
    "### vnet settings\n",
    "vnet_rg     = ws.resource_group  # replace if needed\n",
    "vnet_name   = 'uksouth-vnet'     # replace if needed\n",
    "subnet_name = 'default'          # replace if needed\n",
    "\n",
    "### azure ml names \n",
    "env_name    = f'{name}-dask-env'\n",
    "ct_name     = f'{name}-dask-ct'\n",
    "exp_name    = f'{name}-dask-demo'\n",
    "\n",
    "### trust but verify\n",
    "verify = f'''\n",
    "Name: {name}\n",
    "\n",
    "vNET RG: {vnet_rg}\n",
    "vNET name: {vnet_name}\n",
    "vNET subnet name: {subnet_name}\n",
    "\n",
    "Environment: {env_name}\n",
    "Compute target: {ct_name}\n",
    "Experiment name: {exp_name}\n",
    "'''\n",
    "\n",
    "print(verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cody100-dask-env', '1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if env_name not in ws.environments:\n",
    "    env = Environment.from_existing_conda_environment(env_name, 'dask')\n",
    "    env = env.register(ws)\n",
    "else:\n",
    "    env = ws.environments[env_name]\n",
    "    \n",
    "env.name, env.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute(workspace=Workspace.create(name='uks-azureml', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='copeters-rg'), name=cody100-dask-ct, id=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourceGroups/copeters-rg/providers/Microsoft.MachineLearningServices/workspaces/uks-azureml/computes/cody100-dask-ct, type=AmlCompute, provisioning_state=Succeeded, location=uksouth, tags=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ct_name not in ws.compute_targets:\n",
    "    # create config for Azure ML cluster\n",
    "    # change properties as needed\n",
    "    config = AmlCompute.provisioning_configuration(\n",
    "             vm_size                       = 'STANDARD_D14_V2',  \n",
    "             min_nodes                     = 100,\n",
    "             max_nodes                     = 100,\n",
    "             vnet_resourcegroup_name       = vnet_rg,              \n",
    "             vnet_name                     = vnet_name,            \n",
    "             subnet_name                   = subnet_name,          \n",
    "             idle_seconds_before_scaledown = 300\n",
    "    )\n",
    "    ct = ComputeTarget.create(ws, ct_name, config)\n",
    "    ct.wait_for_completion(show_output=True)    \n",
    "else:\n",
    "    ct = ws.compute_targets[ct_name]\n",
    "    \n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - This compute target type doesn't support non-Docker runs; overriding run configuration enable Docker.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>cody100-dask-demo</td><td>cody100-dask-demo_1579623805_cea9e67b</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/experiments/cody100-dask-demo/runs/cody100-dask-demo_1579623805_cea9e67b?wsid=/subscriptions/6560575d-fa06-4e7d-95fb-f962e74efd7a/resourcegroups/copeters-rg/workspaces/uks-azureml\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: cody100-dask-demo,\n",
       "Id: cody100-dask-demo_1579623805_cea9e67b,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # of nodes \n",
    "nodes = 99\n",
    "exp   = Experiment(ws, exp_name)\n",
    "est   = Estimator('setup', \n",
    "                  compute_target          = ct, \n",
    "                  entry_script            = 'start.py',          # sets up Dask cluster\n",
    "                  environment_definition          = env,                 # use same env as local\n",
    "                  node_count              = nodes,        \n",
    "                  distributed_training    = MpiConfiguration()\n",
    "                 )\n",
    "#run = next(exp.get_runs()) # use this to get existing run (if kernel restarted, etc)\n",
    "run = exp.submit(est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstore = ws.datastores['data4dask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_OPTIONS = {\n",
    "    'account_name': dstore.account_name, \n",
    "    'account_key' : dstore.account_key\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data4uksouth'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstore.account_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qhWtyoChzuJ3PT6YoxXjrQYisoLkDNUBBQiQXNyZvjEuKCKYKHSLjs+x/5Xu0B8D/Ky5VDiFRy5kcKvctjZSRw=='"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dstore.account_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol  = 'abfs'      # use 'adl' for Azure Data Lake Gen 1\n",
    "container = 'datasets'  # only contains ISD, GFS is wip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem(protocol, **STORAGE_OPTIONS, container_name=container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noaa/gfs/GFSProcessed/', 'noaa/gfs/8c6ca145-43b7-4492-8222-30f0a138fe69']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.ls('/noaa/gfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['noaa/gfs/GFSProcessed/year=2019/month=9/day=5/',\n",
       " 'noaa/gfs/GFSProcessed/year=2019/month=9/day=6/',\n",
       " 'noaa/gfs/GFSProcessed/year=2019/month=9/day=7/',\n",
       " 'noaa/gfs/GFSProcessed/year=2019/month=9/day=8/',\n",
       " 'noaa/gfs/GFSProcessed/year=2019/month=9/day=9/']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for file in fs.glob('noaa/gfs/GFSProcessed/year=*/month=*'): \n",
    "    files += fs.ls(f'{file}/')\n",
    "files[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is slow - there are ~50k files\n",
    "files2 = []\n",
    "for file in files: \n",
    "    files2 += fs.glob(f'{file}/*.parquet')\n",
    "files2[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = files2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files) # number of parquet files - there are other random files in there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lz4==2.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install msgpack==0.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port to forward the dask dashboard to on the compute instance\n",
    "# we do not use 8787 because it is already in use \n",
    "dashboard_port = 4242\n",
    "\n",
    "print(\"waiting for scheduler node's ip\")\n",
    "while run.get_status() != 'Canceled' and 'scheduler' not in run.get_metrics():\n",
    "    print('.', end =\"\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if run.get_status() == 'Canceled':\n",
    "    print('\\nRun was canceled')\n",
    "else:\n",
    "    print(f'\\nSetting up port forwarding...')\n",
    "    os.system(f'killall socat') # kill all socat processes - cleans up previous port forward setups \n",
    "    os.system(f'setsid socat tcp-listen:{dashboard_port},reuseaddr,fork tcp:{run.get_metrics()[\"dashboard\"]} &')\n",
    "    print(f'Cluster is ready to use.')\n",
    "\n",
    "c = Client(f'tcp://{run.get_metrics()[\"scheduler\"]}')\n",
    "\n",
    "print(f'\\n\\n{c}')\n",
    "\n",
    "# build the dashboard link \n",
    "dashboard_url = f'https://{socket.gethostname()}-{dashboard_port}.{ws.get_details()[\"location\"]}.instances.azureml.net/status'\n",
    "HTML(f'<a href=\"{dashboard_url}\">Dashboard link</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.restart() # restart Client if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pyarrow in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyarrow) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from pyarrow) (1.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.delayed(dd.read_parquet)(files, engine='pyarrow', storage_options=STORAGE_OPTIONS).compute()\n",
    "%time df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(dd.to_datetime(df.datetime).dt.floor('d'), sorted=False)\n",
    "df = df.persist() \n",
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.describe().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time bites = df.memory_usage(index=True, deep=True).sum().compute()\n",
    "print(f'Dataframe is: {round(bites/1e9, 2)}GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time means = df.groupby(df.index).mean().compute()\n",
    "means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(means.columns):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.style.use('dark_background')\n",
    "    means[col].plot(color='b')\n",
    "    plt.title('Average of {}'.format(col))\n",
    "    plt.xlim([datetime(2008, 1, 1), datetime(2018, 12, 31)])\n",
    "    plt.grid()\n",
    "    \n",
    "    # optionally, log the image to the run\n",
    "    run.log_image(f'mean_{col}', plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the images logged to the run in the studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert any Pandas-like Dask data prep code \n",
    "df['temperature'] = df['temperature']*(9/5)+32 # 'Merica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(npartitions=df.npartitions*10)\n",
    "%time dask.delayed(df.to_parquet)(f'abfs://outputs/noaa/isd_out.parquet', compression='lz4', storage_options=STORAGE_OPTIONS).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.delayed(dd.read_parquet)(files, engine='pyarrow', storage_options=STORAGE_OPTIONS).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for file in fs.glob('noaa/isd/year=*/month=*'): # see https://github.com/dask/adlfs/issues/34\n",
    "    files += fs.ls(f'{file}/')\n",
    "files = [f'{protocol}://{container}/{file}' for file in files if '2019' in file] \n",
    "\n",
    "df2019 = dask.delayed(dd.read_parquet)(files, engine='pyarrow', storage_options=STORAGE_OPTIONS).compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin data prep\n",
    "df = df.fillna(0) \n",
    "df2019 = df2019.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['datetime'].dt.month\n",
    "df2019['month'] = df2019['datetime'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols = [col for col in cols if df.dtypes[col] != 'object' and col not in ['version', 'datetime']]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[col for col in cols if col not in ['temperature']]].persist()\n",
    "y = df.temperature.persist()\n",
    "# end data prep - persist intelligently per https://docs.dask.org/en/latest/best-practices.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(n_estimators=16)\n",
    "%time xgb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_pred = xgb.predict(X).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (((y.to_dask_array().compute()-y_pred)**2).mean())**.5 # runs locally, distribute (?)\n",
    "print(f'Training RMSE: {round(rmse, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df2019[[col for col in cols if col not in ['temperature']]].persist()\n",
    "y_test = df2019.temperature.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_pred = xgb.predict(X_test).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (((y_test.to_dask_array().compute()-y_pred)**2).mean())**.5 # runs locally, distribute (?)\n",
    "print(f'Test RMSE: {round(rmse, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'xgboost_noaa_isd.joblib.dat'\n",
    "joblib.dump(xgb, model_path)\n",
    "xgb = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model.register(ws, model_path, 'xgboost-noaa-isd', \n",
    "                       description='Dask XGBoost NOAA ISD temperature predictor',\n",
    "                       model_framework='XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.close()\n",
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = time.time()\n",
    "print(f'Total run time: {round((t_end-t_start)/60, 2)} minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask2",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
